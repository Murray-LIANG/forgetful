<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Forgetful :/</title>
    <link>http://localhost:1313/forgetful/posts/</link>
    <description>Recent content in Posts on Forgetful :/</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
    <lastBuildDate>Sat, 11 Dec 2021 22:08:35 +0800</lastBuildDate><atom:link href="http://localhost:1313/forgetful/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Designing Data Intensive Applications</title>
      <link>http://localhost:1313/forgetful/2021/12/designing-data-intensive-applications/</link>
      <pubDate>Sat, 11 Dec 2021 22:08:35 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2021/12/designing-data-intensive-applications/</guid>
      <description>Data intensive application Common seen functionalities:
 Store data so that they, or another application, can find it again later (databases) Remember the result of an expensive operation, to speed up reads (caches) Allow users to search data by keyword or filter it in various ways (search indexes) Send a message to another process, to be handled asynchronously (stream processing) Periodically crunch a large amount of accumulated data (batch processing)  Reliability Continuing to work correctly, even when things go wrong.</description>
    </item>
    
    <item>
      <title>Pacemaker Tutorial</title>
      <link>http://localhost:1313/forgetful/2021/11/pacemaker-tutorial/</link>
      <pubDate>Fri, 19 Nov 2021 18:15:34 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2021/11/pacemaker-tutorial/</guid>
      <description>Pacemaker Tutorial MISC CRM Commands 1 2 3 4 5 6 7 8 9  $ # You can set a node to standby mode,  $ # which can be used to simulate a node becoming unavailable, $ # with this command: $ sudo crm node standby NodeName $ # You can change a node’s status from standby  $ # to online with this command: $ sudo crm node online NodeName   </description>
    </item>
    
    <item>
      <title>Interview Questions</title>
      <link>http://localhost:1313/forgetful/2021/11/interview-questions/</link>
      <pubDate>Fri, 12 Nov 2021 16:41:13 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2021/11/interview-questions/</guid>
      <description> 自我介绍 简单介绍项目 经常使用的数据结构有哪些，他们有什么区别，在要用的时候会怎么选 看项目，使用了REST和gRPC，这两个都是API，在使用过程中有什么相同和不同，什么时候用 哪一个 protobuffer和JSON的用处和不同 有没有贡献过社区项目，使用git的话，一般是个什么样的步骤 有没有接触过异步编程，简单说说 比较私人的问题，可以选择不回答，为什么从美国回国找工作  数据结构  数组与链表 排序，及其复杂度  插入：稳定，平均/最差时间复杂度 O(n²)，元素基本有序时最好时间复杂度 O(n)，空间复杂度 O(1)。 希尔：不稳定，平均时间复杂度 O(n^1.3^)，最差时间复杂度 O(n²)，最好时间复杂度 O(n)，空间复杂度 O(1)。 堆：不稳定，时间复杂度 O(nlogn)，空间复杂度 O(1)。 冒泡：稳定，平均/最坏时间复杂度 O(n²)，元素基本有序时最好时间复杂度 O(n)，空间复杂度 O(1)。 快排：不稳定，平均/最好时间复杂度 O(nlogn)，元素基本有序时最坏时间复杂度 O(n²)，空间复杂度 O(logn)。 合并：任何情况时间复杂度都为 O(nlogn)，空间复杂度为 O(n)。   heap/堆如何进行插入和删除的 树，树的遍历  Linux linux interview questions 计算机网络 https://www.eet-china.com/mp/a68780.html  计算机网络体系结构 TCP vs UDP，TCP如何保证可靠性   </description>
    </item>
    
    <item>
      <title>WSL</title>
      <link>http://localhost:1313/forgetful/2021/04/wsl/</link>
      <pubDate>Tue, 20 Apr 2021 23:15:39 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2021/04/wsl/</guid>
      <description>Issues 1. wslregisterdistribution failed with error 0x80080005.  Run PowerShell as admin. .\sc.exe stop LxssManager then .\sc.exe start LxssManager. You may find LxssManager is under STOP_PENDING status after you stop it. .\sc.exe queryex LxssManager  1 2 3 4 5 6 7 8 9 10 11 12  PS C:\WINDOWS\system32&amp;gt; .\sc.exe queryex LxssManager SERVICE_NAME: LxssManager TYPE  : 30 WIN32 STATE : 4 RUNNING (STOPPABLE, NOT_PAUSABLE, IGNORES_SHUTDOWN) WIN32_EXIT_CODE : 0 (0x0) SERVICE_EXIT_CODE : 0 (0x0) CHECKPOINT : 0x0 WAIT_HINT : 0x0 PID : 27192 FLAGS :   Open Task Manger and switch to tab Details.</description>
    </item>
    
    <item>
      <title>Javascript Basic</title>
      <link>http://localhost:1313/forgetful/2021/02/javascript-basic/</link>
      <pubDate>Thu, 18 Feb 2021 15:12:06 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2021/02/javascript-basic/</guid>
      <description>References https://javascript.info Comparison of different types When comparing values of different types, JavaScript converts the values to numbers.
Strict equality === and !== Checks the equality without type conversion. If a and b are of different types, the a === b immediately returns false without an attempt to convert them.
1 2 3  alert(0 == false); // true alert(&amp;#39;&amp;#39; == false); // true alert(0 === false); // false   Comparison with null and undefined 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  alert(null === undefined); // false alert(null == undefined); // true  // null converts to 0, while undefined converts to NaN.</description>
    </item>
    
    <item>
      <title>Nomachine Setup</title>
      <link>http://localhost:1313/forgetful/2020/12/nomachine-setup/</link>
      <pubDate>Thu, 10 Dec 2020 20:13:40 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/12/nomachine-setup/</guid>
      <description>Setup Steps  On CentOS8 machine:  wget https://download.nomachine.com/download/6.12/Linux/nomachine_6.12.3_7_x86_64.rpm sudo dnf install nomachine_6.12.3_7_x86_64.rpm   On Windows machine:  Download nomachine exe installation file from official site: https://www.nomachine.com/download . Install it.   Enable 3440x1440 resolution on CentOS8:   gtf 3440 1440 60
Sample output:
1 2  # 3440x1440 @ 60.00 Hz (GTF) hsync: 89.40 kHz; pclk: 419.11 MHz Modeline &amp;#34;3440x1440_60.00&amp;#34; 419.11 3440 3688 4064 4688 1440 1441 1444 1490 -HSync +Vsync     Copy above output, then run xrandr --newmode &amp;quot;3440x1440_60.</description>
    </item>
    
    <item>
      <title>Reactive Programming, RX and RxJava</title>
      <link>http://localhost:1313/forgetful/2020/12/reactive-programming-rxjava/</link>
      <pubDate>Tue, 01 Dec 2020 11:21:10 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/12/reactive-programming-rxjava/</guid>
      <description>Reactive programming Reactive means acting in response to a situation rather than creating or controlling it: reacting to it. Reactive programming is actually similar to this definition, meaning writing code that reacts to changes.
It’s actually a way of coding with asynchronous data streams that will make it easier for us to code apps and interfaces that respond dynamically to changes in data. Meaning that whenever we have changes in the data, our app will respond reactively to those changes.</description>
    </item>
    
    <item>
      <title>Python Asynchronous Programming</title>
      <link>http://localhost:1313/forgetful/2020/11/python-async-programming/</link>
      <pubDate>Mon, 30 Nov 2020 11:49:59 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/11/python-async-programming/</guid>
      <description>References  深入理解Python异步编程(上)  how-does-asyncio-actually-work  Example: Chain coroutines    Codes git/py-asyncio-play</description>
    </item>
    
    <item>
      <title>Python Module Entry Points</title>
      <link>http://localhost:1313/forgetful/2020/09/python-entry-points/</link>
      <pubDate>Tue, 22 Sep 2020 15:53:50 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/09/python-entry-points/</guid>
      <description>Python Module Entry Points You could use pkg_resources to get the full list of entry points.
1 2 3 4 5 6  for ep in pkg_resources.iter_entry_points(&amp;#39;manila.scheduler.filters&amp;#39;): print(ep.name, ep.dist.location, ep.module_name) # Sample output AvailabilityZoneFilter /opt/stack/manila manila.scheduler.filters.availability_zone ShareGroupReplicationFilter /opt/stack/manila manila.scheduler.filters.share_group_filters.share_group_replication   manila.scheduler.filters here is called group or namespace. You can get it via package name as below:
1 2 3 4 5 6  for e in pkg_resources.working_set: if e.key == &amp;#39;manila&amp;#39;: print(e.</description>
    </item>
    
    <item>
      <title>OpenStack Manila Share Group Replication Dev Tips</title>
      <link>http://localhost:1313/forgetful/2020/06/openstack-manila-share-group-replication-dev-tips/</link>
      <pubDate>Tue, 23 Jun 2020 15:45:28 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/06/openstack-manila-share-group-replication-dev-tips/</guid>
      <description>When Share Group Replication Creation Succeed    Share
Replication
Type create_replica
/delete_replica
Implemented Group
Replication
Type create_share
_group_replica
Implemented Succeed in share group replication creation Note     write/read/dr/None yes/no None yes or no no, raise error from share_group/api.    write/read/dr/None yes/no write/read/dr yes yes    write/read/dr yes write/read/dr no yes, the default implementation is creating replicas for shares in the group.</description>
    </item>
    
    <item>
      <title>OpenStack Manila</title>
      <link>http://localhost:1313/forgetful/2020/05/openstack-manila/</link>
      <pubDate>Thu, 28 May 2020 16:43:24 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/05/openstack-manila/</guid>
      <description>Below commands are used by elab. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  $ # remove admin_subnet and admin_net created by devstack $ openstack subnet delete admin_subnet $ openstack network delete admin_net $ VLAN_ID=50 $ NETWORK_NAME=unity-net $ SUBNET_NAME=unity-subnet $ SUBNET_RANGE=$(grep &amp;#39;^FIXED_RANGE&amp;#39; /tmp/dg-local.conf|head -1|cut -f2 -d=) $ $ # create datamover network $ NETWORK_ID=$(openstack network create --share -f value -c id --provider-network-type vlan --provider-physical-network public_eth1 --provider-segment $VLAN_ID $NETWORK_NAME) $ SUBNET_ID=$(openstack subnet create -f value -c id --ip-version 4 --network $NETWORK_NAME --subnet-range $SUBNET_RANGE $SUBNET_NAME) $ $ # create security service with Domain Controller information, make sure this DC is also in the VLAN $ SECURITY_NAME=unity-ad $ AD_IP=12.</description>
    </item>
    
    <item>
      <title>SCOM and SNMP</title>
      <link>http://localhost:1313/forgetful/2020/04/scom-snmp/</link>
      <pubDate>Wed, 29 Apr 2020 10:50:52 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/04/scom-snmp/</guid>
      <description>References  SCOM 2019 installation: https://www.prajwaldesai.com/scom-1801-install-guide/   Integration with Unity SNMP Agent Steps   Install snmp agent on a Windows 10 machine: 10.245.54.154. The snmp agent is configured to manage Unity: 10.245.101.39.
  The SCOM is deployed on a Windows Server 2019 machine: 10.245.54.155.
  Configure SCOM to discover snmp device.
3.1 Create a Discovery Rule.  3.2 After a rule is created, a new network device could be discovered.</description>
    </item>
    
    <item>
      <title>SSH Tunnel</title>
      <link>http://localhost:1313/forgetful/2020/04/ssh-tunnel/</link>
      <pubDate>Mon, 13 Apr 2020 21:08:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/04/ssh-tunnel/</guid>
      <description>On ubuntu-client1 1 2 3  # nohup ssh -N root@127.0.0.1 \ -L 10.245.48.66:29418:review.openstack.org:29418 \ -L 10.245.48.66:4 43:review.openstack.org:443   </description>
    </item>
    
    <item>
      <title>Rabin-Karp Algorithm</title>
      <link>http://localhost:1313/forgetful/2020/02/algorithm-rabin-karp/</link>
      <pubDate>Mon, 10 Feb 2020 20:12:06 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/algorithm-rabin-karp/</guid>
      <description>Used to find a substring of a string, i.e. finding nan in banana.
General idea:
 Calculate the hash value for nan, hash1 = n * 26**2 + a * 26 + n.  NOTE: Need to mod the hash value with a large prime number, then the hash value won&amp;rsquo;t go too large.
From the beginning of string banana, calculate the hash value for 3 chars, ban, hash2 = b * 26**2 + a * 26 + n.</description>
    </item>
    
    <item>
      <title>Basic Math Knowledge</title>
      <link>http://localhost:1313/forgetful/2020/02/ml-math-basic/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/ml-math-basic/</guid>
      <description>平均数是一个统计学概念，期望是一个概率论概念。
平均数是实验后根据实际结果统计得到的样本的平均值，期望是实验前根据概率分布预测的样本的平均值。
之所以说预测是因为在实验前能得到的期望与实际实验得到的样本的平均数总会不可避免地存在偏差，毕竟随机实验的结果永远充满着不确定性。如果我们能进行无穷次随机实验并计算出其样本的平均数的话，那么这个平均数其实就是期望。当然实际上根本不可能进行无穷次实验，但是实验样本的平均数会随着实验样本的增多越来越接近期望，就像频率随着实验样本的增多会越来越接近概率一样如果说概率是频率随样本趋于无穷的极限那么期望就是平均数随样本趋于无穷的极限。
作者：奇诺城堡 链接：https://www.zhihu.com/question/25391960/answer/39780528 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。  </description>
    </item>
    
    <item>
      <title>Block Storage and File Storage</title>
      <link>http://localhost:1313/forgetful/2020/02/block-storage-file-storage/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/block-storage-file-storage/</guid>
      <description>Block Storage   tgt framework could be used to implement a tgt server in user space.
  You could customize a new backend store type which could, for example, send the SCSI message in customized format via a socket. Then a user space program listening on the socket could deserialize from customized format and do some additional tasks on the SCSI message like multi-write them to different replicas.</description>
    </item>
    
    <item>
      <title>C&#43;&#43; Stack Overflow</title>
      <link>http://localhost:1313/forgetful/2020/02/c-stack-overflow/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/c-stack-overflow/</guid>
      <description>在函数的递归调用中，函数中变量所占的空间要直到递归结束才能被释放，这样函数不停的递归，堆栈早晚会被用完。
 解决递归调用堆栈溢出问题，一种方法是在递归函数中每次动态的分配变量的内存，在使用结束的时候释放内存。以二维数组的动态分配为例：  1 2 3 4 5 6 7 8 9 10 11  p=new double*[1000]; for (int m=0;m&amp;lt;1000;m++) { p[m]=new double[5000]; } for(int n=0;n&amp;lt;1000;n++) { delete[] p[n]; } delete[] p;    解决递归调用堆栈溢出问题，另外一种方法是在定义递归函数时，输入变量地址，通过指针操作，而非变量本身参加函数的递归调用，不会不断占用堆栈空间不释放。
  堆栈的大小只有1M，如果在函数中定义了一个占用内存比较大的变量，那么也会导致堆栈溢出。
这种情况只需在定义的时候定义为静态变量就行了，因为静态变量是不占用堆栈内存的。如：
1 2 3 4  void main（） { int a[10010010]； }   在函数内定义的变量默认auto类型，也就是栈变量，运行时使用的是栈空间，函数结束后自动清理返回内存。这里在函数内定义如此大的一个数组，已经超过了单个函数可使用的最大栈空间，也会提示stack overflow。解决办法是将其定义为static int型的静态变量，这样就不占用栈空间了。
1 2 3 4  void main（） { static int a[10010010]； }     </description>
    </item>
    
    <item>
      <title>Command `ip route`</title>
      <link>http://localhost:1313/forgetful/2020/02/network-ip-route/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/network-ip-route/</guid>
      <description>Display a routing table with ip route show http://linux-ip.net/html/tools-ip-route.html 1 2 3 4 5 6 7 8 9 10  [root@tristan]# route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 192.168.99.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0 127.0.0.0 0.0.0.0 255.0.0.0 U 0 0 0 lo 0.0.0.0 192.168.99.254 0.0.0.0 UG 0 0 0 eth0 [root@tristan]# ip route show 192.168.99.0/24 dev eth0 scope link 127.0.0.0/8 dev lo scope link default via 192.</description>
    </item>
    
    <item>
      <title>Create a Self Signed Certificate</title>
      <link>http://localhost:1313/forgetful/2020/02/openssl-certificate/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/openssl-certificate/</guid>
      <description>Create root CA (Do this once) Create key for root CA 1 2 3  # Generate the private key for CA, which is used to sign certificates from others. # Remove `-des3` if dont want to protect the key with password. openssl genrsa -des3 -out root_ca.key 4096   Create root CA certificate and self sign it 1 2  openssl req -x509 -new -nodes -key root_ca.key -sha256 -days 1024 -out root_ca.</description>
    </item>
    
    <item>
      <title>Design Learning: Cassandra Best Practices at eBay</title>
      <link>http://localhost:1313/forgetful/2020/02/design-cassandra-best-practices-ebay/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/design-cassandra-best-practices-ebay/</guid>
      <description>https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/ </description>
    </item>
    
    <item>
      <title>Design News Feed System</title>
      <link>http://localhost:1313/forgetful/2020/02/design-news-feed/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/design-news-feed/</guid>
      <description>http://blog.gainlo.co/index.php/2016/03/29/design-news-feed-system-part-1-system-design-interview-questions/ Requirements Features  CRUD posts Comment on posts Share posts  What is in a news feed post?  Author Content Media (photos, videos, .etc) Comments and replies Operations:  CRUD Comment and reply on posts Share posts    What is on news feed page?  Sequence of posts Query or filter posts Operations:  Load more posts Delete posts I don&amp;rsquo;t want to see    Estimations (scale requirements)  How many users?</description>
    </item>
    
    <item>
      <title>Distributed</title>
      <link>http://localhost:1313/forgetful/2020/02/distributed/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/distributed/</guid>
      <description>Partitioning / Sharding github.com/Murray-LIANG/nodering
Distributed Lock  Could use etcd as backend. Some implementation: https://github.com/etcd-io/etcd/blob/master/clientv3/concurrency/mutex.go  If two clients request to lock at the same time, the later client would change the value of same key in etcd3 but with new revision. The lock logic needs to wait for old revision deletion which means the older lock is released. To make sure locks released even when clients crash, use expiration on the key.</description>
    </item>
    
    <item>
      <title>Docker Tips</title>
      <link>http://localhost:1313/forgetful/2020/02/docker-tips/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/docker-tips/</guid>
      <description>Cannot Resolve Host Name inside Running Containers 1 2 3 4 5 6 7  sudo vim /etc/docker/daemon.json { &amp;#34;dns&amp;#34;: [&amp;#34;10.245.177.15&amp;#34;] } sudo systemctl restart docker.service   </description>
    </item>
    
    <item>
      <title>Erasure Code</title>
      <link>http://localhost:1313/forgetful/2020/02/erasure-code/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/erasure-code/</guid>
      <description>通信行业中有三种编码容错技术：
 检错码：仅具备识别错码功能 而无纠正错码功能； 纠错码：不仅具备识别错码功能，同时具备纠正错码功能； 纠删码：则不仅具备识别错码和纠正错码的功能，而且当错码超过纠正范围时，还可把无法纠错的信息删除。  纠删码主要应用在网络传输中避免包的丢失，存储系统利用它来提高存储可靠性。相比多副本复制而言，纠删码能够以更小的数据冗余度获得更高数据可靠性，但编码方式较复杂，需要大量计算。 纠删码只能容忍数据丢失，无法容忍数据篡改，纠删码正是得名与此。
目前，纠删码技术在分布式存储系统中的应用主要有三类：
 阵列纠删码（Array Code: RAID5、RAID6等） RS(Reed-Solomon)里德-所罗门类纠删码 LDPC(LowDensity Parity Check Code)低密度奇偶校验纠删码。  RAID是EC的特殊情况。在传统的RAID中，仅支持有限的磁盘失效，RAID5只支持一个盘失效，RAID6支持两个盘失效，而EC支持多个盘失效。
RS m = n + k
n为原始数据块的个数，k为校验块的个数，总数据块个数为m。
 最大容忍k个数据丢失 重建时至少需要n个数据块，n值越大，重建时需要拷贝数据越多 k取值大，故障容忍度高，取值小，数据冗余低  </description>
    </item>
    
    <item>
      <title>Etcd</title>
      <link>http://localhost:1313/forgetful/2020/02/etcd/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/etcd/</guid>
      <description>etcd is a highly available key-value store.
redis, memcached are popular key-value stores. These are general-purpose distributed memory caching system often used to speed up dynamic database-driven websites by caching data and objects in memory.
etcd cannot be stored in memory, they can only be persisted in disk storage, whereas redis can be cached in ram and can also be persisted in disk.
etcd does not have various data types. But redis and other key-value stores have data-type flexibility.</description>
    </item>
    
    <item>
      <title>Fix the git error on CentOS</title>
      <link>http://localhost:1313/forgetful/2020/02/fix-centos-git-certificate-error/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/fix-centos-git-certificate-error/</guid>
      <description>The error is related to corp certificate.
 sudo yum install -y ca-certificates download the emc certificates from: https://eos2git.cec.lab.emc.com/liangr/corp-notes/blob/master/emc_cert/emc_cacert.crt  cp emc_cacert.crt /etc/pki/ca-trust/source/anchors/ run command: update-ca-trust extract  </description>
    </item>
    
    <item>
      <title>Forward Proxy vs Reverse Proxy</title>
      <link>http://localhost:1313/forgetful/2020/02/network-forward-and-reverse-proxy/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/network-forward-and-reverse-proxy/</guid>
      <description>Forward Proxy: Acting on behalf of a requestor (or service consumer) Reverse Proxy: Acting on behalf of service/content producer.  </description>
    </item>
    
    <item>
      <title>From Etcd v2 to v3</title>
      <link>http://localhost:1313/forgetful/2020/02/etcd-v2-to-v3/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/etcd-v2-to-v3/</guid>
      <description>In etcd3, the base server interface uses gRPC instead of JSON for increased efficiency. Support for JSON endpoints is maintained through a gRPC gateway. The new API revisits the design of key expiry TTLs, replacing them with lightweight streaming lease keepalive model. Watchers are redesigned as well, replacing the older event model with one that streams and multiplexes events over key intervals. The v3 data model does away with explicit key hierarchies and unreliable watch windows, replacing them with a flat binary key space with transactional, multiversion concurrency control semantics.</description>
    </item>
    
    <item>
      <title>Glance</title>
      <link>http://localhost:1313/forgetful/2020/02/openstack-glance/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/openstack-glance/</guid>
      <description>glance-api Dispatch the REST request.
glance-registry Process the request about the metadata of image.
Storage backend Configed in /etc/glance/glance-api.conf.
Misc Two processes run for Glance:
 g-api g-reg  </description>
    </item>
    
    <item>
      <title>Golang Comparable `==`</title>
      <link>http://localhost:1313/forgetful/2020/02/golang-comparable/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/golang-comparable/</guid>
      <description>Golang中的类型   基本类型：整型（int/uint/int8/uint8/int16/uint16/int32/uint32/int64/uint64/byte/rune等）、浮点数（float32/float64）、复数类型（complex64/complex128）、字符串（string）。
  复合类型（又叫聚合类型）：数组和结构体类型。
  引用类型：切片（slice）、map、channel、指针。
  接口类型：如error。
  ==操作的两个操作数类型必须相同，否则编译报错。 复合类型：数组和结构体 数组的长度为类型的一部分，长度不同意味着类型不同，不能比。
 数组：比每个元素，对于不同类型的元素再根据类型递归比下去。 结构体：比每个字段。  引用 比两个引用变量是否指向同一份数据，并不比较实际指向的数据内容。
引用类型中的切片类型和map类型不可比较，只能和nil比较
切片 1 2 3 4 5 6  a := [] interface {}{ 1 , 2.0 } a[1] = a // 切片间接指向自己 fmt.Println(a) // runtime: goroutine stack exceeds 1000000000-byte limit // fatal error: stack overflow   为什么切片不能比较？
可能因为：
 切片和数组类似，但是切片是引用，比较的是是否指向同一份数据，而这种比较方式与数组比较元素的方式完全不一样，容易让程序员混淆。 如果将切片做成比较元素的方式，长度和容量是切片的隐藏字段，长度和容量不一样的切片会采用与数组一样的方式，被认为是类型不一样而编译错误。这样，在大部分情况下，切片的比较都会编译失败。 即使忽略长度和容量不一样，切片元素可以是自身的引用（即切片本身），这样比较会导致堆栈溢出。  所以，golang不让你比较切片。</description>
    </item>
    
    <item>
      <title>Golang init</title>
      <link>http://localhost:1313/forgetful/2020/02/golang-init/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/golang-init/</guid>
      <description>Initializing Packages on Import 1 2 3  func init() { rand.Seed(time.Now().UnixNano()) }   Above init codes ensure the initialization are performed prior to the package being used.
Multiple Instances of init() The init() function can be declared multiple times throughout a package.
In most cases, init() functions will execute in the order that you encounter them.
You can put init() in several files. They will be compiled in the alphabetical order of file names.</description>
    </item>
    
    <item>
      <title>Golang Interview Questions</title>
      <link>http://localhost:1313/forgetful/2020/02/golang-interview-questions/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/golang-interview-questions/</guid>
      <description>Explain what is go routine in GO? How you can stop go routine? A goroutine is a function which is capable of running concurrently with other functions.
To stop goroutine, you pass the goroutine a signal channel, that signal channel is used to push a value into when you want the goroutine to stop. The goroutine polls that channel regularly as soon as it detects a signal, it quits.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  Quit := make(chan bool) go func() { for { select { case &amp;lt;- quit: return default // do other stuff  } } }() // Do stuff  // Quit goroutine Quit &amp;lt;- true   Why would you prefer to use an empty struct{}?</description>
    </item>
    
    <item>
      <title>Golang Panic and Recover</title>
      <link>http://localhost:1313/forgetful/2020/02/golang-panic-recover/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/golang-panic-recover/</guid>
      <description>Panic Panics are similar to C++ and Java exceptions, but are only intended for run-time errors, such as following a nil pointer or attempting to index an array out of bounds.
A panic stops the normal execution of a goroutine:
 When a program panics, it immediately starts to unwind the call stack. This continues until the program crashes and prints a stack trace, or until the built-in recover function is called.</description>
    </item>
    
    <item>
      <title>Golang Stack Traces</title>
      <link>http://localhost:1313/forgetful/2020/02/golang-stack-traces/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/golang-stack-traces/</guid>
      <description>https://www.ardanlabs.com/blog/2015/01/stack-traces-in-go.html 1 2 3 4 5 6 7 8 9 10  package main func main() { slice := make([]string, 2, 4) Example(slice, &amp;#34;hello&amp;#34;, 10) } func Example(slice []string, str string, i int) { panic(&amp;#34;Want stack trace&amp;#34;) }   You will get a stack track like this:
1 2 3 4 5 6 7  01 goroutine 1 [running]: 02 main.Example(0x2080c3f50, 0x2, 0x4, 0x425c0, 0x5, 0xa) /Users/bill/Spaces/Go/Projects/src/github.com/goinaction/code/ temp/main.go:9 +0x64 03 main.</description>
    </item>
    
    <item>
      <title>Golang Summer of Code</title>
      <link>http://localhost:1313/forgetful/2020/02/golang-soc/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/golang-soc/</guid>
      <description>Week 1 1. N-Way Merge Sort Given a int64 array with 16M unsorted numbers, use multiple goroutine to sort this array. Your implementation should be faster that the Quick Sort in single thread.
Check list:
 Pass unit tests. Pass test cases. Analysis the bottle-neck via Go Profile.  Week 2 1. Read MIT MapReduce Course and Complete Excercise. 2. Report 10 Most Frequent URLs from A Huge Set of URLs Given a file containing a huge set of URLs, use MapReduce and Shuffle to get the 10 most frequent URLs and their frequency.</description>
    </item>
    
    <item>
      <title>Golang Tips</title>
      <link>http://localhost:1313/forgetful/2020/02/golang-tips/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/golang-tips/</guid>
      <description>Some Tips of Golang Context https://siadat.github.io/post/context Context package was initially designed to implement:
 Request cancellation Deadline  Instead of forcing a function to stop, the caller should inform it that its work is no longer needed. Caller sends the information about cancellation and let the function decide how to deal with it. For example, a function could clean up and return early when it is informed that its work is no longer needed.</description>
    </item>
    
    <item>
      <title>Goroutines</title>
      <link>http://localhost:1313/forgetful/2020/02/golang-goroutine/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/golang-goroutine/</guid>
      <description>Goroutines are a way of doing tasks concurrently in golang. They allow us to create and run multiple methods or functions concurrently in the same address space inexpensively.
Goroutines are lightweight abstractions over threads because their creation and destruction are very cheap as compared to threads, and they are scheduled over OS threads.
Goroutines vs. Threads Goroutines are NOT any faster than threads.
Memory consumption The creation of Goroutines require much less memory as compared to threads.</description>
    </item>
    
    <item>
      <title>GVIM Settings on Windows</title>
      <link>http://localhost:1313/forgetful/2020/02/gvim-windows-settings/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/gvim-windows-settings/</guid>
      <description>Copy the vimrc from devenv git repo to Windows home directory, name it _gvimrc The reason to give the name _gvimrc is to avoid the vim in git bash to load it. This would let the vim in git bash lighter and faster.
Run below commands in git bash (copied from all_set.sh of devenv) 1 2 3 4 5 6  mkdir -p ~/.vim/autoload ~/.vim/bundle ~/.vim/colors \  &amp;amp;&amp;amp; git clone -q https://github.</description>
    </item>
    
    <item>
      <title>High Availability - HAProxy and Keepalived</title>
      <link>http://localhost:1313/forgetful/2020/02/high-availability-haproxy-keepalived/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/high-availability-haproxy-keepalived/</guid>
      <description>HAProxy 1 2 3 4 5 6 7 8 9 10 11 12 13 14  # file /etc/haproxy/haproxy.cfg ... ... frontend www bind load_balancer_anchor_IP:80 # to which the floating IP binds default_backend nginx_pool backend nginx_pool balance roundrobin mode tcp # below two are the App Servers server web1 web_server_1_private_IP:80 check server web2 web_server_2_private_IP:80 check   Keepalived 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42  # file /etc/keepalived/keepalived.</description>
    </item>
    
    <item>
      <title>How to Approach a System Design Interview Question</title>
      <link>http://localhost:1313/forgetful/2020/02/design-how-to-approach-a-system-design-interview-question/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/design-how-to-approach-a-system-design-interview-question/</guid>
      <description>The system design interview is an open-ended conversation. You are expected to lead it.
Step 1: Outline use cases, constraints, and assumptions. Gather requirements and scope the problem. Ask questions to clarify use cases and constraints. Discuss assumptions.
Question examples:
 Who is going to use it? How are they going to use it? How many users are there? What does the system do? What are the inputs and outputs of the system?</description>
    </item>
    
    <item>
      <title>How to Source Openrc with Zsh</title>
      <link>http://localhost:1313/forgetful/2020/02/openstack-source-openrc-with-zsh/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/openstack-source-openrc-with-zsh/</guid>
      <description>Create file openrc_zsh with content:  1 2 3 4 5 6 7 8 9 10 11  # Single quotes of &amp;#39;EOF&amp;#39; disable the expansion of $ in the function sourceopenrc. cat &amp;gt; ~/devstack/openrc_zsh &amp;lt;&amp;lt;&amp;#39;EOF&amp;#39; #!/bin/env bash function sourceopenrc { pushd ~/devstack &amp;gt;/dev/null eval $(bash -c &amp;#34;. openrc $1 $2 &amp;amp;&amp;gt;/dev/null;env|sed -n &amp;#39;/OS_/ { s/^/export /;p}&amp;#39;&amp;#34;) popd &amp;gt;/dev/null } sourceopenrc $1 $2 EOF   Run source openrc_zsh admin admin.</description>
    </item>
    
    <item>
      <title>Increase the Max LUNs of FC Host Adapter</title>
      <link>http://localhost:1313/forgetful/2020/02/increase-fc-max-luns-on-host/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/increase-fc-max-luns-on-host/</guid>
      <description>In our environment, Emulex LightPulse Fibre Channel Host Adapter (lpfc) is used. For other adapters like QLogic Corp Fibre Channel to PCI Express HBA (qla driver) and Cisco Systems Inc VIC FCoE Host Adapter (fnic driver), please refer to https://access.redhat.com/solutions/26017 or ask for help from RedHat experts directly.
Default value of max LUNs For lpfc, the default value of max LUNs is 255.
1 2  stack@ubuntu-server7:~$ cat /sys/class/scsi_host/host9/lpfc_max_luns 255   If a lun is with HLU number greater than 255, then the host cannot see this lun.</description>
    </item>
    
    <item>
      <title>Install Storops RPM on RHEL7</title>
      <link>http://localhost:1313/forgetful/2020/02/openstack-redhat-install-storops/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/openstack-redhat-install-storops/</guid>
      <description>Install a RHEL VM.   If using qcow2 image, the default user is cloud-user. Use the OpenStack ssh key to login.
  Create a new user stack with password welcome.
  1 2 3  $ sudo useradd stack $ sudo passwd stack $ sudo visudo   Modify /etc/ssh/sshd_config to allow login via password  Entry name is PasswordAuthentication.
Register subscription 1 2 3 4 5 6 7  $ sudo subscription-manager register $ sudo subscription-manager attach --auto $ sudo subscription-manager repos \  --enable=rhel-7-server-rpms --enable=rhel-7-server-extras-rpms \  --enable=rhel-7-server-rh-common-rpms \  --enable=rhel-ha-for-rhel-7-server-rpms $ sudo yum update   Download storops rpm from github 1  curl -OJL https://github.</description>
    </item>
    
    <item>
      <title>IPv6</title>
      <link>http://localhost:1313/forgetful/2020/02/network-ipv6/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/network-ipv6/</guid>
      <description>IPv4 addresses are 32 bits long and IPv6 addresses are 128 bits long.
Classifying IPv6 Addresses IPv6 has three types of addresses:
 Unicast: An IPv6 unicast address is used to identify a single interface. Packets sent to a unicast address are delivered to that specific interface. Anycast: IPv6 anycast addresses identify groups of interfaces, which typically belong to different nodes. Packets destined to an anycast address are sent to the nearest interface in the group, as determined by the active routing protocols.</description>
    </item>
    
    <item>
      <title>Kubernetes and CSI</title>
      <link>http://localhost:1313/forgetful/2020/02/kubernetes-and-csi/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/kubernetes-and-csi/</guid>
      <description>基本概念 Plugin In the CSI world, it means a service that exposes gRPC endpoints. There are two plugins:
 Node plugin - a gRPC server that needs to run on the Node where the volume will be provisioned. Controller plugin - a gRPC server that can run anywhere (even on the master).  implementing three interfaces:
 Node interface Controller interface Identity interface  Plugin Deployment The Node and Controller plugin could be released as one binary or in two binaries.</description>
    </item>
    
    <item>
      <title>Kubernetes API Server</title>
      <link>http://localhost:1313/forgetful/2020/02/kubernetes-api-server/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/kubernetes-api-server/</guid>
      <description>Although kubelet could watch a directory and run pods found configured under that directory, but in a Kubernetes cluster, kubelet will get most its pods to run from the Kubernetes API server.
Kubernetes stores all its cluster state in etcd, a distributed data store with a strong consistency model. This state includes what nodes exist in the cluster, what pods should be running, which nodes they are running on, and a whole lot more.</description>
    </item>
    
    <item>
      <title>Kubernetes CSI Unity</title>
      <link>http://localhost:1313/forgetful/2020/02/kubernetes-csi-unity/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/kubernetes-csi-unity/</guid>
      <description>Setup K8S Nodes 1. Create three VMs. 2. Create the user k8s with password welcome, add it to sudo group. 3. Add corp CA certs on all nodes. https://eos2git.cec.lab.emc.com/liangr/corp-notes/blob/master/fix-pip-emc-cert.md 4. Install docker.io on all nodes. NOTE: Do NOT use 18.09.2 currently (2019-04-25). It causes below errors:
1 2 3 4 5 6  root@k8s-master:~# docker pull k8s.gcr.io/kube-apiserver:v1.14.1 v1.14.1: Pulling from kube-apiserver 346aee5ea5bc: Pulling fs layer 7f0e834d5a94: Pulling fs layer error pulling image configuration: Get https://storage.</description>
    </item>
    
    <item>
      <title>Kubernetes Demo</title>
      <link>http://localhost:1313/forgetful/2020/02/kubernetes-demo/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/kubernetes-demo/</guid>
      <description>Pre-requisites   Pre-configurations on Unity.
  Create a Pool and a NasServer.
NOTE: the NFSv4 needs to be enabled on the NasServer.
  Create hosts for the K8S nodes, k8s-node1 and k8s-node2 and manually register the initiators. Use below command to view the initiator name:
  1 2  $ cat /etc/iscsi/initiatorname.iscsi InitiatorName=iqn.&amp;lt;rest of host iscsi initiator&amp;gt;     The docker daemon on all Nodes in K8S should be configured with DNS.</description>
    </item>
    
    <item>
      <title>Kubernetes Introduction</title>
      <link>http://localhost:1313/forgetful/2020/02/kubernetes-intro/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/kubernetes-intro/</guid>
      <description>基本概念 Cluster 各种资源，计算、存储、网络的集合。
Master 调度应用放在哪里运行。为了保证高可用，可以部署多个Master。
Node 由Master管理，负责监控并汇报容器的状态。根据Master的要求，对容器进行Life Cycle Management。
Pod K8S中最小的工作单元。每一个Pod可以包含一个或者多个容器。Pod中的容器作为一个整体被Master调度到一个Node上运行。
为什么需要提出Pod这一层抽象呢？因为
  存在多个容器天生需要紧密联系，一起工作，Pod比容器抽象更高，将多个容器封装在一个部署单元中，易于管理，比如以Pod为最小单位进行调度、扩展、资源共享、LCM。
  Pod中的所有容器使用同一个网络的namespace，即相同的IP地址和Port空间。除了网络，还有共享的存储资源。
  Controller K8S不会直接创建Pod，而是通过Controller来管理Pod的。Controller中定义了Pod的部署特性，比如有多少个副本，在什么样的Node上面运行。K8S提供了多种Controller,包括Deployment、ReplicaSet、DaemonSet、StatefulSet、Job等。
Deployment 可以通过它来部署应用，Deployment来管理Pod的多个副本，并保证Pod安装预期的状态运行。
ReplicaSet Pod的多副本管理，使用Deployment会间接自动创建ReplicaSet。
DaemonSet 用于每个Node只运行一个Pod副本的情形。
StatefulSet 能保证Pod的每个副本在整个生命周期中名称是不变的。使用其他Controll当某个Pod发生故障需要删除并重新启动，Pod的名称会变化。除此之外，StatefulSet会保证副本按照固定的顺序启动，更新或者删除。
Job 运行结束就删除。其他方式部署的Pod会一直存在。
Service Deployment部署多个Pod，每个Pod有自己的IP，外界不能通过Pod的IP来访问应用，因为Pod的IP会随着Pod的重启而变化。因此，用Service定义外界访问一组特定Pod的方式。Service有自己的IP和Port，而且还为Pod提供Load Balance。
Controller和Service分别控制着Pod的运行和访问。
Namespace 如果多个用户或者项目组共享使用一个物理的Cluster，如何隔离他们创建的Controller和Pod呢？
使用Namespace可以将一个物理的Cluster划分成多个逻辑的Cluster，每个Cluster在单独的Namespace中，他们的资源完全隔离。
K8S创建了两个默认的Namespace，default和kube-system，后者用于放置K8S自己创建的系统资源。
kubelet, kubeadm, kubectl  kubelet - 运行在所有节点上，负责启动Pod和容器。 kubeadm - 用于初始化Cluster。 kubectl - K8S命令行工具，用于部署管理应用，查看管理各种资源。  Installation Create three VMs. Create the user k8s with password welcome, add it to sudo group. Add corp CA certs on all nodes.</description>
    </item>
    
    <item>
      <title>Kubernetes Kubelet</title>
      <link>http://localhost:1313/forgetful/2020/02/kubernetes-kubelet/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/kubernetes-kubelet/</guid>
      <description>kubelet is the lowest level component in Kubernetes. It&amp;rsquo;s responsible for what&amp;rsquo;s running on an individual machine. You can think of it as a process watcher like supervisord, but focused on running containers. It has one job: given a set of containers to run, make sure they are all running.
There are a few ways the kubelet finds pods to run:
 a directory it polls for new pod manifests to run a URL it polls and downloads pod manifests from from the Kubernetes API server  The first one above is the simplest: to run a pod, we just put a manifest file in the watched directory.</description>
    </item>
    
    <item>
      <title>Kubernetes Networking</title>
      <link>http://localhost:1313/forgetful/2020/02/kubernetes-networking/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/kubernetes-networking/</guid>
      <description>Different Networking Model  Container-to-container Pod-to-pod Pod-to-service Internet-to-service  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  +-------------------------------------------------------------------+ +-------------------------------------------------------------------+ | | | | | +-----------------------------------+ | | +-----------------------------------+ | | | +-------------+ +-------------+ | | | | +-------------+ +-------------+ | | | | | Container.</description>
    </item>
    
    <item>
      <title>Kubernetes Overlay Network</title>
      <link>http://localhost:1313/forgetful/2020/02/kubernetes-overlay-network/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/kubernetes-overlay-network/</guid>
      <description>Overlay networks such as vxlan or ipsec encapsulate the packet into another packet. This makes entities addressable that are outside of the scope of another machine.
Any solution on L2 or L3 makes a pod addressable on the network. This means a pod is reachable not just within the Docker network, but is directly addressable from outside the Docker network. These could be public or private IP addresses.
The way of route and iptables described here is a L3 solution (not a overlay networking) making a pod addressable on the network.</description>
    </item>
    
    <item>
      <title>Kubernetes What is Pause Container</title>
      <link>http://localhost:1313/forgetful/2020/02/kubernetes-what-is-pause-container/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/kubernetes-what-is-pause-container/</guid>
      <description>Why We Need Pods Docker supports containers, which are great for deploying single units of software. However, this model can become a bit cumbersome when you want to run multiple pieces of software together. You often see this when developers create Docker images that use supervisord as an entrypoint to start and manage multiple processes. For production systems, many have found that it is instead more useful to deploy those applications in groups of containers that are partially isolated and partially share on environment.</description>
    </item>
    
    <item>
      <title>Learning Notes of Machine Learning in Action(机器学习实战)</title>
      <link>http://localhost:1313/forgetful/2020/02/ml-basic-knowledge/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/ml-basic-knowledge/</guid>
      <description>机器学习基础 机器学习就是把无序的数据转换成有用的信息。
统计学 机器学习用到了统计学知识。现实世界中存在很多例子（人类行为），我们无法为之建立精确的数学模型，而为了解决这类问题，我们就需要统计学工具。
术语  监督学习算法 必须知道预测什么，例如目标变量的分类信息。分类和回归都属于监督学习。回归：主要用于预测数值型数据。一个例子是数据拟合曲线：通过给定数据点的最优拟合曲线。 无监督学习算法 不给定目标值。聚类和密度估计属于此类。聚类：将数据集合分成由类似的对象组成的多个类的过程。将寻找描述数据统计值的过程称为密度估计，即估计数据与每个分组的相似程度。  算法的选择 一般并不存在最好的算法或者可以给出最好结果的算法。发现最好算法的关键环节是反复试错的迭代过程。
开发机器学习应用程序的步骤  收集数据 准备输入数据：完成基本数据转换 分析输入数据：需人工干预，确保数据集中没有垃圾数据，一般采用二维或者三维展示数据，例如使用matplotlib。 训练算法：无监督学习，一般不需要训练算法，主要步骤在测试算法中。 测试算法：使用测试数据集测试算法。对于监督学习，必须预先知道用于评估算法的目标变量值，便可以计算出算法的成功或者错误率。对于无监督学习，可以采用其他评测手段来检测算法成功率。 使用算法  K-邻近算法  常用的分类算法之一 并不需要训练算法 适用于数值型和标称型数据  优点  精度高 对异常值不敏感 无数据输入假定  缺点  计算复杂度高 空间复杂度高  信息增益 Information Gain 在划分数据集之前之后信息发生的变化称为信息增益。通过计算每个特征值划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择。
熵 Entropy 集合信息的度量方式称为香农熵或者简称为熵。熵定义为信息的期望值：
1  H = - SUM(p(Xi) * log2 p(Xi)), p(Xi)为选择某一分类的概率   熵越高，则混合的数据也越多。
决策树  适用于数值型和标称型  优点  计算复杂度不高 输出结果易于理解 对中间值缺失不敏感 可以处理不相关特征数据  缺点  可能产生过度匹配问题  步骤  收集数据 准备数据：决策树的构造算法只支持标称型数据，因此数值型数据必须离散化。 分析数据：可以通过matplotlib绘出决策树，检查图形是否符合预期。 训练算法：构造决策树的数据结构，经过选取最优特征（熵），划分数据集，构造决策树等具体步骤，输出决策树，类似： {&#39;no surfacing&#39;: {0: &#39;no&#39;, 1: {&#39;flippers&#39;: {0: &#39;no&#39;, 1: &#39;yes&#39;}}}} 测试算法 使用算法  NOTE  每次挑选获取信息增益最大的特征，来划分分类。 每次使用特征划分数据集之后，得到的若干个子数据集中会把该特征去掉，这样保证了递归算法终将结束。 如果划分得到的子数据集中仍然有多个分类，取类别出现频率最高的作为该子集的类别。 过度匹配（Overfitting）：决策树一般能非常好的匹配数据，然而树过于深，分支多造成匹配选项可能太多了。为了解决过度匹配的问题，可以裁剪决策树，如果叶子节点只能增加少许信息，则可以删除该节点，将其并入其他叶子节点中。 为了节约每次都重新构造决策树的时间，一般会把树的信息pickle到文件系统中。  </description>
    </item>
    
    <item>
      <title>Linux Bridge and VLAN</title>
      <link>http://localhost:1313/forgetful/2020/02/linux_bridge_and_vlan/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/linux_bridge_and_vlan/</guid>
      <description>What is Linux Bridge? There is a VM on the Host. And only one ethernet port on the Host, named eth0.
If bind eth0 to the VM, then the Host and other new VMs have no port to access the internet.
So the solution is to create a bridge on the Host, name it br0. Assign a virtual card to VM, name it vnet0. br0 connects the vnet0 and eth0.</description>
    </item>
    
    <item>
      <title>Linux Interview Questions</title>
      <link>http://localhost:1313/forgetful/2020/02/linux-interview-questions/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/linux-interview-questions/</guid>
      <description>查看系统负载的常用命令 1 2 3 4 5 6  [root@centos6 ~ 10:56 #37]# w 10:57:38 up 14 min, 1 user, load average: 0.00, 0.00, 0.00 USER TTY FROM LOGIN@ IDLE JCPU PCPU WHAT root pts/0 192.168.147.1 18:44 0.00s 0.10s 0.00s w [root@centos6 ~ 10:57 #38]# uptime 10:57:47 up 14 min, 1 user, load average: 0.00, 0.00, 0.00   其中load average即系统负载，三个数值分别表示一分钟、五分钟、十五分钟内系统的平均负载，即平均任务数。
linux系统里，知道buffer和cache如何区分吗？ buffer和cache都是内存中的一块区域，当CPU需要写数据到磁盘时，由于磁盘速度比较慢，所以CPU先把数据存进buffer，然后CPU去执行其他任务，buffer中的数据会定期写入磁盘；当CPU需要从磁盘读入数据时，由于磁盘速度比较慢，可以把即将用到的数据提前存入cache，CPU直接从Cache中拿数据要快的多。
常用的查看系统资源的命令 top, free
 VIRT虚拟内存用量 RES物理内存用量 SHR共享内存用量 %MEM内存用量  查看当前系统进程 ps -aux</description>
    </item>
    
    <item>
      <title>Microservices Architecture</title>
      <link>http://localhost:1313/forgetful/2020/02/microservices-architecture/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/microservices-architecture/</guid>
      <description>Service oriented architecture (SOA) resulted in monolithic large services.
Microservices knows how to size a service.
Design Principles High Cohesion  Single thing done well Single focus  Identify a single focus. It could be:
 Business function which has its own clear inputs and outputs. Business domain where the microservice focus is in the form of creating, retrieving, updating, deleting (CRUD) data related to a specific part of the organization.</description>
    </item>
    
    <item>
      <title>Multi-Threading and Multi-Processing</title>
      <link>http://localhost:1313/forgetful/2020/02/operating-system-multithreading-multiprocessing/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/operating-system-multithreading-multiprocessing/</guid>
      <description>Multithreading refers to the general task of running more that one thread of execution within an operating system. Multithreading is more generically called multiprocessing, which can include multiple system processes (a simple example on Windows would be, e.g., running Internet Explorer and Microsoft Word at the same time), or it can consist of one process that has multiple threads within it.
Multithreading (or should I say, multiprocessing) is a software concept.</description>
    </item>
    
    <item>
      <title>Network Packages Forwarding</title>
      <link>http://localhost:1313/forgetful/2020/02/network-package-forwarding/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/network-package-forwarding/</guid>
      <description>NAT Network Address Translation: 在一个网络内部，自定义合法的ip地址。内网各主机通过内网通讯；与外网通过NAT转换，变成外网合法ip。这样，将内网与外网隔离，各个网络有自己的ip，既可以重叠，又可以通过少数几个ip与外网通讯，在ip大量缺乏的现代，节省了很多。
Packages Forwarding   PC1要访问 www.google.com ，需要先知道对应IP地址。域名只起助记作用，互联网访问通过IP进行。
  于是，PC1需要像DNS请求，查找 www.google.com 对应的ip，即发送dns请求：PC1查找dns，发现不在同一个网络，不同网段需要网关转发。但是，PC1需要先发送给网关，就需要先知道网关ip。网关用于连接不同网络，并且有自己的IP，PC1需要知道网关ip。于是，通过ARP请求，像内网广播网关ip，网关回复mac地址。PC1得到了网关的mac地址，将ip包封装到以太网帧，发送给网关。
  网关收到该以太网帧，需要转交给dns服务器。同样，网关可能需要发送ARP请求，得到dns的mac地址。
  dns服务器收到请求，将 www.google.com 的ip发送给网关，网关再根据NAT会话表项，将目的ip转换成PC1的，再发送给PC1（此过程可能同样需要ARP请求）。
  PC1收到了目的ip，再可以通过类似上面的方式发送请求（目的ip再可以直接填上获取的ip）。
  Example of Sending a Request to a Remote Server for a Web Page The example is using 5 layered TCP/IP network model.
  On the application layer (layer #5), the browser initiates an HTTP connection request message to the remote host, www.</description>
    </item>
    
    <item>
      <title>Non-Blocking IO in Go</title>
      <link>http://localhost:1313/forgetful/2020/02/golang-non-blocking-io/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/golang-non-blocking-io/</guid>
      <description>If you are using Go you are probably using non-blocking IO.
What is non-blocking IO? A simple explanation: It allows you to read() and write() to a file descriptor (that is, any type of open file be it a socket, pipe, a file on disk, whatever) without having these calls block just because the file is not ready.
It&amp;rsquo;s just something like this:
1 2 3 4  fd, _ := syscall.</description>
    </item>
    
    <item>
      <title>Notes of fastai ml1 lesson 1</title>
      <link>http://localhost:1313/forgetful/2020/02/ml-fastai-ml1-lesson1/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/ml-fastai-ml1-lesson1/</guid>
      <description>Links  https://github.com/fastai/fastai/blob/master/courses/ml1/lesson1-rf.ipynb  https://www.youtube.com/watch?v=CzdWqFTmn0Y&amp;feature=youtu.be  https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-1-84a1dc2b5236    </description>
    </item>
    
    <item>
      <title>NVMe and Related</title>
      <link>http://localhost:1313/forgetful/2020/02/nvme/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/nvme/</guid>
      <description>Storage Evolution  SRAM  Latency: 1X Size of Data: 1X   DRAM  Latency: ~10X Size of Data: ~100X   3D XPoint  Latency: ~100X Size of Data: ~1000X   NAND  Latency: ~100000X Size of Data: ~1000X   HDD  Latency: ~10 MillionX Size of Data: ~10000X    NVMe NVM Express is a standardized high performance software interface for PCI Express Solid State Drives.</description>
    </item>
    
    <item>
      <title>Object Storage</title>
      <link>http://localhost:1313/forgetful/2020/02/object-storage/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/object-storage/</guid>
      <description>Object Storage vs Block vs File 块设备相对于操作系统来说就是block device，裸硬盘，提供最基本的IO Read、IO Write等操作，操作系统将block device映射到/dev目录，一般应用并不会直接访问块设备，一般先创建分区，然后格式化成文件系统，最后挂载到某个本地目录mount point来使用。
而文件系统存储，一般实现了Posix接口标准，直接提供文件系统的接口，如open，read，write，close，mkdir，walk，chown，chmod等等。文件是通过树的形式组织的，具有明显的层次结构。文件的访问需要从根目录一直到文件所在的位置。
而对象存储，通过HTTP RESTful的接口如GET，POST，PUT，DELETE来管理一些非结构化的数据，比如图片，文档。它没有目录的概念。
对象存储设备 从存储进化方向来看：Block &amp;raquo;&amp;gt; NAS &amp;raquo;&amp;gt; Object Storage，使得客户端越来越瘦，在Block时代，每个客户端需要使用存储，需要自己实现文件管理的功能，后来把这些功能移到存储服务器上便有了NAS。现如今，把对象管理的功能移到存储服务器上便有了Object Storage。
 数据存储 智能分布 每个对象的元数据管理。对象元数据与filesystem中的inode类似，只不过是存放这对象的大小、数据块等信息。  元数据服务器 控制着整个对象存储的元数据，例如对象的分布情况，每个对象在那个存储设备上，等。
对象存储读访问流程  客户端发出读请求。 元数据服务器给出对象所在的存储设备。 客户端直接向该设备读取对象。 存储设备通过对象ID计算出对象所在的file或者block，读取出数据返回给客户端。 客户端接收到存储设备返回的数据，可以将对象跟存储设备的对应关系cache下来。  对象存储写访问流程  客户端发出写请求。 元数据服务器根据当前各个存储设备的情况计算出，或者直接使用一致性hash算出对象应该存放在哪个存储设备。 客户端直接向该设备写入对象。 存储设备存放对象并将对象ID，所在的file或者block，等元数据记录下来。 客户端成功接收存储设备返回的信息，并记录在cache中。  </description>
    </item>
    
    <item>
      <title>Oh My Zsh Tips</title>
      <link>http://localhost:1313/forgetful/2020/02/oh-my-zsh-tips/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/oh-my-zsh-tips/</guid>
      <description>Installation  sudo apt install -y zsh sh -c &amp;quot;$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&amp;quot; git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions  Customized Configuration  In ~/.zshrc, set ZSH_THEME to robbyrussell. Modify plugins to: 1 2 3 4  plugins=( git zsh-autosuggestions )    Append below lines to ~/.zshrc. 1 2 3 4 5 6 7 8 9 10  export ZSH_AUTOSUGGEST_HIGHLIGHT_STYLE=&amp;#39;fg=10&amp;#39; export GOPATH=~/git/go export PATH=$PATH:$GOPATH/bin:/snap/bin # Jupyter notebook alias inote=&amp;#39;cd /mnt/hgfs/liangr-win10/git/ipython &amp;amp;&amp;amp; \ nohup /home/liangr/git/ml/venv/py36/bin/jupyter notebook \ --ip 0.</description>
    </item>
    
    <item>
      <title>OpenStack Attach Volume</title>
      <link>http://localhost:1313/forgetful/2020/02/openstack-attach/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/openstack-attach/</guid>
      <description>Sample of REST Request and Response 1  stack@rocky:~/cinder$ openstack --debug server add volume vm-ml-1 liangr   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57  START with options: [u&amp;#39;--debug&amp;#39;, u&amp;#39;server&amp;#39;, u&amp;#39;add&amp;#39;, u&amp;#39;volume&amp;#39;, u&amp;#39;vm-ml-1&amp;#39;, u&amp;#39;liangr&amp;#39;] command: server add volume -&amp;gt; openstackclient.</description>
    </item>
    
    <item>
      <title>OpenStack Cinder Multiple Backends</title>
      <link>http://localhost:1313/forgetful/2020/02/openstack-cinder-unity-multi-backends-test/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/openstack-cinder-unity-multi-backends-test/</guid>
      <description>Cinder configuration Some settings in /ect/cinder/cinder.conf
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  [DEFAULT] ... ... enabled_backends = unity_backend_1,unity_backend_2 ... ... [unity_backend_1] storage_protocol = iSCSI san_ip = 10.245.101.39 san_login = admin san_password = Password123! volume_driver = cinder.volume.drivers.dell_emc.unity.Driver volume_backend_name = unity_backend_1 force_delete_attached_snapshots = True [unity_backend_2] storage_protocol = iSCSI san_ip = 192.168.1.58 san_login = admin san_password = Password123!</description>
    </item>
    
    <item>
      <title>OpenStack Curl Command Examples</title>
      <link>http://localhost:1313/forgetful/2020/02/openstack-curl-rest-examples/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/openstack-curl-rest-examples/</guid>
      <description>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  # create volume curl -g -i \  -X POST http://192.168.77.63/volume/v2/2a97f77210a54665ba9df8a505df6624/volumes \  -H &amp;#34;User-Agent: python-cinderclient&amp;#34; \  -H &amp;#34;Content-Type: application/json&amp;#34; \  -H &amp;#34;Accept: application/json&amp;#34; \  -H &amp;#34;X-Auth-Token: {SHA1}5b729489a9259aae07eb591361c474bdad29e71d&amp;#34; \  -d &amp;#39; {&amp;#34;volume&amp;#34;: {&amp;#34;status&amp;#34;: &amp;#34;creating&amp;#34;, &amp;#34;user_id&amp;#34;: null, &amp;#34;name&amp;#34;: &amp;#34;v-m-3&amp;#34;, &amp;#34;imageRef&amp;#34;: null, &amp;#34;availability_zone&amp;#34;: null, &amp;#34;description&amp;#34;: null, &amp;#34;multiattach&amp;#34;: false, &amp;#34;attach_status&amp;#34;: &amp;#34;detached&amp;#34;, &amp;#34;volume_type&amp;#34;: null, &amp;#34;metadata&amp;#34;: {}, &amp;#34;consistencygroup_id&amp;#34;: null, &amp;#34;source_volid&amp;#34;: null, &amp;#34;snapshot_id&amp;#34;: null, &amp;#34;project_id&amp;#34;: null, &amp;#34;source_replica&amp;#34;: null, &amp;#34;size&amp;#34;: 1 } }&amp;#39;   1 2 3 4 5 6 7 8 9 10 11 12 13 14  # force detach curl -g -i \  -X POST \  http://192.</description>
    </item>
    
    <item>
      <title>OpenStack Deployment using Kolla Ansible</title>
      <link>http://localhost:1313/forgetful/2020/02/openstack-kolla-ansible/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/openstack-kolla-ansible/</guid>
      <description>Network layout API network  eth3 of ubuntu-serverX: 192.168.4.X/24 Internal VIP: 192.168.4.254  External network   eth0 of ubuntu-serverX.
NOTE: 172.30.X.X/16 subnet is used as the floating IP of OpenStack VMs. Do NOT use 192.168.1.X/24 subnet. Refer to Network settings
  External VIP: 192.168.1.254, used to access Horizon from Windows Client. Because the eth0 of Neutron node (aka. ubuntu-server3) is bound to br-ex, IP 192.168.1.254 need to be set to br-ex manually after deploy, so as IP 192.</description>
    </item>
    
    <item>
      <title>OpenStack Dev Tips</title>
      <link>http://localhost:1313/forgetful/2020/02/openstack-dev/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/openstack-dev/</guid>
      <description>Add release notes 1 2 3 4 5 6  ---fixes:- |Dell EMC Unity Driver: Fixes `bug 1798529 &amp;lt;https://bugs.launchpad.net/cinder/+bug/1798529&amp;gt;`_ to add an option for force deleting the snapshot even if it is attached to hosts.  Check storops version 1 2  import pkg_resources pkg_resources.get_distribution(&amp;#34;storops&amp;#34;).version   Or
1  $ python -c &amp;#34;import pkg_resources; print(pkg_resources.get_distribution(&amp;#39;storops&amp;#39;).version)&amp;#34;   </description>
    </item>
    
    <item>
      <title>OpenStack Devstack Tips</title>
      <link>http://localhost:1313/forgetful/2020/02/openstack-devstack/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/openstack-devstack/</guid>
      <description>Clean Up   Unstack and clean up the databases, .etc.
~/devstack/clean.sh
  Uninstall all python packages via pip.
1 2 3 4 5 6  sudo pip freeze | grep -v &amp;#34;git+https&amp;#34; | \  xargs sudo pip uninstall -y for i in cinder glance keystone neutron nova; do bash -c &amp;#34;cd /opt/stack/$i&amp;amp;&amp;amp; sudo python setup.py develop --uninstall&amp;#34;; done     [Optional] Remove all installed binaries and source codes.</description>
    </item>
    
    <item>
      <title>OpenStack Faulty Device</title>
      <link>http://localhost:1313/forgetful/2020/02/openstack-faulty-device/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/openstack-faulty-device/</guid>
      <description>References  https://www.cnblogs.com/sting2me/p/8849689.html  https://www.cnblogs.com/sting2me/p/8888420.html   Faulty device的产生 What are faulty devices? os-brick module is mainly used to connect/disconnect the devices.
Interface connect_volume is used to discover the block devices of storage systems, disconnect_volume is for cleaning the block devices.
When the multipath tool on hosts cannot access the host path, it marks the device as faulty.
How are faulty devices generated? The faulty devices show up under the race condition occurs between connect_volume and disconnect_volume.</description>
    </item>
    
    <item>
      <title>OpenStack Generic Group and Consistent Group</title>
      <link>http://localhost:1313/forgetful/2020/02/openstack-cg/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/openstack-cg/</guid>
      <description>References https://docs.openstack.org/cinder/latest/admin/blockstorage-groups.html https://docs.openstack.org/cinder/latest/contributor/groups.html Consistent Group For a group to support consistent group snapshot, the group specs in the corresponding group type should have the following entry:
1  {&amp;#39;consistent_group_snapshot_enabled&amp;#39;: &amp;lt;is&amp;gt; True}   Similarly, for a volume to be in a group that supports consistent group snapshots, the volume type extra specs would also have the following entry:
1  {&amp;#39;consistent_group_snapshot_enabled&amp;#39;: &amp;lt;is&amp;gt; True}   Cinder Cli Microversion  The minimum microversion to support group type and group specs is 3.</description>
    </item>
    
    <item>
      <title>OpenStack Install libpq-dev on Ubuntu18/Mint19</title>
      <link>http://localhost:1313/forgetful/2020/02/openstack-libpq-dev-install/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/openstack-libpq-dev-install/</guid>
      <description>Meet errors when running tox -e py27,pep8 of Cinder If meet below error, need to install libpq-dev with version 9.5.14.
Error: could not determine PostgreSQL version from &amp;lsquo;10.6&amp;rsquo;</description>
    </item>
    
    <item>
      <title>OpenStack Introduction</title>
      <link>http://localhost:1313/forgetful/2020/02/openstack-intro/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/openstack-intro/</guid>
      <description>Main Services  Nova Neutron Glance Cinder Keystone Swift (Optional)  Nodes Control Node Used to manage the OpenStack env.
Keystone, Glance, Horizon services run on it.
Some Nova management module, Neutron management module, SQL database, Message Queue, and NTP services run on it.
Network Node Provides network including L2 and L3, like route, NAT, DHCP, and etc.
Main Neutron services run on it.
Storage Node Provides storage, like Block storage and Object storage.</description>
    </item>
    
    <item>
      <title>OpenStack Keystone</title>
      <link>http://localhost:1313/forgetful/2020/02/openstack-keystone/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/openstack-keystone/</guid>
      <description>Manage the Users and Roles. Manage the Endpoints of services. Authentication and authorization.  Key points User Credentials Authentication Token Project, Tenant OpenStack resources belong to the Project/Tenant, that is, the resouces are project based not user based. Every user including admin must be a member of the project before accessing the resources.
Service Endpoint Each service exposes several endpoints (URL). Other service sends commands to the endpoint.
1 2  # list endpoints $ openstack catalog list   Role Keystone stores a list of roles.</description>
    </item>
    
    <item>
      <title>OpenStack Multi-attach</title>
      <link>http://localhost:1313/forgetful/2020/02/openstack-multiattach/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/openstack-multiattach/</guid>
      <description>References https://docs.openstack.org/nova/latest/admin/manage-volumes.html#volume-multi-attach https://docs.openstack.org/cinder/latest/admin/blockstorage-volume-multiattach.html http://specs.openstack.org/openstack/cinder-specs/specs/ocata/add-new-attach-apis.html Versions Needed  The minimum required compute API microversion for attaching a multiattach-capable volume to more than one server is 2.60. Cinder 12.0.0 (Queens) or newer is required. The nova-compute service must be running at least Queens release level code (17.0.0) and the hypervisor driver must support attaching block storage devices to more than one guest. Refer to Feature Support Matrix for details on which compute drivers support volume multiattach.</description>
    </item>
    
    <item>
      <title>OpenStack Neutron</title>
      <link>http://localhost:1313/forgetful/2020/02/openstack-neutron/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/openstack-neutron/</guid>
      <description>Software Defined Network
Functionality L2 switching Virtual switch: support Linux Bridge and Open vSwitch. Based on the Linux Bridge and OVS, Neutron supports VLAN. Besides, Tunneling overlay network like VxLan and GRE are supported.
L3 routing Virtual router: leverages IP forwarding and iptables to do routing and NAT.
Load balancing Firewall Key points Network  Local: Instance can only access to the instances on the same node. Flat: no VLAN tagging.</description>
    </item>
    
    <item>
      <title>OpenStack Nova</title>
      <link>http://localhost:1313/forgetful/2020/02/openstack-nova/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/openstack-nova/</guid>
      <description>Services  nova-api nova-scheduler nova-compute nova-conductor Hypervisor  Where the services run Generally, the api, scheduler, conductor are running on Control Node. compute, Hypervisor are running on Compute Node.
General flow  User (End user or other services) sends requests to nova-api to create a VM instance. nova-api sends message to Message Queue to ask nova-scheduler to schedule a Compute Node to create the VM. nova-scheduler gets the message and pick a Compute Node via a filter algorithm, then sends message to let that Compute Node (A) to create VM.</description>
    </item>
    
    <item>
      <title>OpenStack Nova Attach Process Logs of Newton Version</title>
      <link>http://localhost:1313/forgetful/2020/02/openstack-attach-logs/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/openstack-attach-logs/</guid>
      <description>1. Reserve the VM 1.1 Get a lock on VM by nova.compute.manager.do_reserve 1.2 Update the DB via conductor 1.3 Release the lock by nova.compute.manager.do_reserve 2. Get a lock on VM by nova.compute.manager.do_attach_volume (will released in the end) 3. GET call to cinder to get information of volume 3.1 Print a log Attaching volume 5ed0cc5c-158c-4857-9bd5-3556884a23f0 to /dev/vdb 3.2 REST request to cinder 1 2 3 4  REQ: curl -g -i -X GET \  http://172.</description>
    </item>
    
    <item>
      <title>OpenStack Nova Detach Process Logs of Newton Version</title>
      <link>http://localhost:1313/forgetful/2020/02/openstack-detach-logs/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/openstack-detach-logs/</guid>
      <description>1. Get connector properties (219ms) 1 2 3 4 5 6 7  get_connector_properties: call { &amp;#39;execute&amp;#39;: None, &amp;#39;my_ip&amp;#39;: &amp;#39;172.16.1.11&amp;#39;, &amp;#39;enforce_multipath&amp;#39;: True, &amp;#39;host&amp;#39;: &amp;#39;newton&amp;#39;, &amp;#39;root_helper&amp;#39;: &amp;#39;sudo nova-rootwrap /etc/nova/rootwrap.conf&amp;#39;, &amp;#39;multipath&amp;#39;: True}   1.1 Check multipathd status, if multipath-tools is not installed or down, exception raises 1 2 3 4 5 6  $ multipathd show status path checker states: up 3 paths: 2 busy: False   1.2 Get the initiator name 1 2 3 4 5 6 7  $ cat /etc/iscsi/initiatorname.</description>
    </item>
    
    <item>
      <title>OpenStack ospt Tips</title>
      <link>http://localhost:1313/forgetful/2020/02/openstack-ospt-tips/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/openstack-ospt-tips/</guid>
      <description>SSH to the VM where ospt is installed 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  $ ssh guest@10.245.48.66 # use password: welcome $ ssh stack@ubuntu-perf-test # use password: welcome $ cd ~/perf-test # Replace the &amp;lt;mgmt_ip&amp;gt; in `ospt_*.sh`. # create hosts and luns. $ ./ospt_create.sh # it outputs a `create-servers.log` and `create-volumes.log` where you can see the time used.</description>
    </item>
    
    <item>
      <title>OpenStack Setup Tenant</title>
      <link>http://localhost:1313/forgetful/2020/02/openstack-setup-tenant/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/openstack-setup-tenant/</guid>
      <description>Prerequisites  Have the account to the OpenStack env.  Steps Login to the OpenStack GUI. Open 192.168.1.254 in Chrome or other web browser.
[Do for the first time] Setup the Networks  Navigate Project -&amp;gt; Network -&amp;gt; Networks, click Create Network. Input the network name and create a Subnet. Suggest using private subnets like: 172.16.*.* to 172.30.*.*. Input the DNS servers.     Navigate Project -&amp;gt; Network -&amp;gt; Routers, click Create Router.</description>
    </item>
    
    <item>
      <title>OpenStack Thick Volume</title>
      <link>http://localhost:1313/forgetful/2020/02/openstack-thick-volume/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/openstack-thick-volume/</guid>
      <description>1 2  openstack volume type create thick --property provisioning:type=&amp;#39;thick&amp;#39; \  --property thick_provisioning_support=&amp;#39;&amp;lt;is&amp;gt; True&amp;#39;   </description>
    </item>
    
    <item>
      <title>OpenStack Triage Tips</title>
      <link>http://localhost:1313/forgetful/2020/02/openstack-triage-tips/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/openstack-triage-tips/</guid>
      <description>Get the versions of OpenStack and Drivers 1 2 3 4 5 6 7  $ find . -name &amp;#34;*&amp;#34; -type f | xargs grep -n -P -i &amp;#34;starting.*&amp;#34; | grep -v &amp;#34;eventlet&amp;#34; /var/log/cinder/scheduler.log:4490:2018-03-03 00:13:06.689 3581 INFO cinder.service [-] Starting cinder-scheduler node (version 9.1.4) /var/log/cinder/scheduler.log:8000:2018-09-06 21:04:58.062 974844 INFO cinder.service [-] Starting cinder-scheduler node (version 9.1.4) /var/log/cinder/volume.log:25398:2018-09-06 21:03:36.372 969733 INFO cinder.service [-] Starting cinder-volume node (version 9.1.4) /var/log/cinder/volume.log:25434:2018-09-06 21:03:36.406 969733 INFO cinder.</description>
    </item>
    
    <item>
      <title>OpenStack VNX Driver LUN SP Ownership Load Balance</title>
      <link>http://localhost:1313/forgetful/2020/02/openstack-vnx-sp-auto-assign/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/openstack-vnx-sp-auto-assign/</guid>
      <description>There is an option -aa to load balance the ownership of LUNs. Some answers on EMC community https://community.emc.com/thread/114798?start=0&amp;tstart=0 </description>
    </item>
    
    <item>
      <title>Openvswitch Bridge Auto Boot with Ubuntu 16.04</title>
      <link>http://localhost:1313/forgetful/2020/02/openvswitch-auto-start/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/openvswitch-auto-start/</guid>
      <description>The bridges of ovs are down after reboot the host. We need to run the commands manually to bring the bridges up and set IP to it.
This page gives out the workaround.
Here lists the summary of the workaround.
It writes Ubuntu16.10 fixes this issue. So this is for 16.04.
 Install openvswitch-switch tool. NOTE: it will remove all the bridges which were created by kolla-ansible. Edit the config file: /lib/systemd/system/openvswitch-nonetwork.</description>
    </item>
    
    <item>
      <title>OSP12 Containerization Tips</title>
      <link>http://localhost:1313/forgetful/2020/02/osp12-containerization/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/osp12-containerization/</guid>
      <description>Install the undercloud according to official doc  NOTE: The files/folders used below are under ~/templates
Edit the openstack-tripleo-heat-templates/environments/docker.yaml to enable the docker image for cinder/manila 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  git diff openstack-tripleo-heat-templates/environments/ diff --git a/openstack-tripleo-heat-templates/environments/docker.yaml b/openstack-tripleo-heat-templates/environments/docker.yaml index 223a179..faef160 100644 --- a/openstack-tripleo-heat-templates/environments/docker.yaml +++ b/openstack-tripleo-heat-templates/environments/docker.yaml @@ -51,14 +51,18 @@ resource_registry:  OS::TripleO::Services::Horizon: .</description>
    </item>
    
    <item>
      <title>OSP13 Deployment TripleO Tips</title>
      <link>http://localhost:1313/forgetful/2020/02/osp13-tripleo/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/osp13-tripleo/</guid>
      <description>Verify Quick Fixes Executes below commands on undercloud 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61  # Enter OpenStack env source ~/stackrc # Install the rpms which contain the latest fixes sudo rpm -Uvh \  openstack-tripleo-heat-templates-8.</description>
    </item>
    
    <item>
      <title>OSP14 Configure Network for Manila</title>
      <link>http://localhost:1313/forgetful/2020/02/osp14-configure-network-for-manila/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/osp14-configure-network-for-manila/</guid>
      <description>OSP Network Topology  Environment The undercloud is 192.168.1.202.
The overcloud is deployed on ubuntu-server2 (compute node) and ubuntu-server10 (control node).
Ethernet interfaces on ubuntu-server10 are:
 enp8s0f0: enp8s0f1: eth1, used for overcloud tenant network. enp8s0f2: eth0, used for OSP Provision/Ctrl Plane network. eno1: eth2, not used. enp8s0f3: ens2f1: eth3, connected to br-ex, used to External network.  Ethernet interfaces on ubuntu-server2 are:
 TBD  Undercloud uses 192.168.139.* subnet to dhcp overcloud nodes.</description>
    </item>
    
    <item>
      <title>Package Storops RPM on RHEL</title>
      <link>http://localhost:1313/forgetful/2020/02/openstack-package-storops-rpm/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/openstack-package-storops-rpm/</guid>
      <description>Install a RHEL VM.   If using qcow2 image, the default user is cloud-user. Use the OpenStack ssh key to login.
  Create a new user stack with password welcome.
  1 2 3  $ sudo stack $ sudo passwd stack $ sudo visudo   Register subscription 1 2  $ sudo subscription-manager register $ sudo subscription-manager attach --auto   Install the basic packages like rpm-build 1  $ sudo yum install -y rpm-build git wget python-pip   NOTE: need to upgrade the setuptools using pip.</description>
    </item>
    
    <item>
      <title>Pass Through FC LUN from Host to VM</title>
      <link>http://localhost:1313/forgetful/2020/02/pass-through-fc-lun-from-host-to-vm/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/pass-through-fc-lun-from-host-to-vm/</guid>
      <description>https://www.ibm.com/developerworks/linux/library/l-npiv-kvm/index.html </description>
    </item>
    
    <item>
      <title>Pip Download and Install Pandas</title>
      <link>http://localhost:1313/forgetful/2020/02/pip-download-install-pandas/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/pip-download-install-pandas/</guid>
      <description>Pip Download and Install 1 2 3 4 5 6  $ # Download $ pip download --platform win_amd64 --only-binary=:all: --python-version 37 --implementation cp --abi cp37m pandas $ # Install $ pip install --no-index --find-links=. pandas   </description>
    </item>
    
    <item>
      <title>Prefer IPv4 on Ubuntu</title>
      <link>http://localhost:1313/forgetful/2020/02/prefer-ipv4-on-ubuntu/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/prefer-ipv4-on-ubuntu/</guid>
      <description>Edit /etc/gai.conf and locate the line and uncomment it:
1  precedence ::ffff:0:0/96 100   </description>
    </item>
    
    <item>
      <title>Processes and Threads</title>
      <link>http://localhost:1313/forgetful/2020/02/operating-system-processes-threads/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/operating-system-processes-threads/</guid>
      <description>Processes and Threads 进程是某个程序的一次运行活动，是资源分配和调度的一个独立单位。线程是进程中的一个控制线 程，最小的独立运行的基本单位。进程中所有线程共享进程的资源，如虚拟地址空间，文件描述符 。每个线程有自己的程序计数器，寄存器和栈等。
Multiprocessing and Multithreading  单进程，单线程，MS-DOS大致是这种 多进程，单线程，多数Unix及Linux是这种 多进程，多线程，Windows，Solaris 2.x和OS/2是这种 单进程，多线程，VxWorks是这种  Thread的种类 User Level Threads 仅存于用户空间中。对于他们的创建，撤销，之间的同步和通信等功能无需系统调用来实现。在同一个进程中 的用户级线程切换不需要内核支持，而内核完全不知道用户线程的存在，而是视作一个进程，所以调 度是以进程为单位进行的。
 优点   线程切换不需要转到内核空间，节省了宝贵的内核空间 调度算法进程专用，由用户程序制定   缺点   系统调用阻塞，同一个进程一个线程阻塞，整个进程都会阻塞 一个进程只能在一个CPU上获得执行   就线程的同时执行而言，任意给定时刻每个进程只能有一个线程在运行，而且只有一个处理器内核会被分配给该进 程。对于一个进程，可能有成千上万个用户级进程，但是他们对系统资源没有影响。运行时库调度并分派这些 线程。库调度器从进程中的多个线程中选择一个线程，然后该线程和该进程允许的一个内核线程关联起来。
Kernel Supported Threads 内核空间为每个内核支持的线程设置了一个线程控制块，内核则根据该控制块来感知线程的存在并控制。
所以，
 优点   在多核处理器上，内核可以调用同一个进程中的多个线程同时工作 如果一个进程中哦某个线程阻塞了，其他线程仍然可以得到工作   缺点   相对于用户线程来说，切换开销大，需要从用户态，进入内核态并且由内核进行切换，因为线程调度和管 理都在内核实现。   内核级线程是内核对象，常驻内核空间。用户线程与之一一对应。运行时 库会为每个用户线程分配一个内核级线 程。
混合方式  结合了以上两种线程。库和操作系统都可以管理线程。进程有着自己的内核线程池。可运行的用户线程由运行 时库分派并标记为准备好执行的可用线程。操作系统选择用户线程并将它映射到线程池中的可用内核线程。
内核线程不会被销毁或者重建，只是在必要时被分配给不同的用户级线程，而不是每当创建新的用户级线程都会创 建新的内核线程。</description>
    </item>
    
    <item>
      <title>Prometheus</title>
      <link>http://localhost:1313/forgetful/2020/02/prometheus/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/prometheus/</guid>
      <description>https://prometheus.io/docs/instrumenting/exporters/ Unity Exporter Ideas We could develop an exporter for Unity. An prometheus exporter works as a server and prometheus will scrape the metrics from it in a specified interval.
 Develop an exporter for Unity. Register some real time queries for some common resource (or supporting configured via conf file) When prometheus scrapes the metrics, Collect method of Unity exporter will be called. It will get the latest query result from Unity in the logic of Collect.</description>
    </item>
    
    <item>
      <title>Python Concurrent Module</title>
      <link>http://localhost:1313/forgetful/2020/02/python-concurrent/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/python-concurrent/</guid>
      <description>GIL会使得Python程序无法通过多线程实现并行计算，IO密集型稍微好一些，因为IO blocking会释放GIL，但是计算密集型无法通过多线程充分利用多核CPU的优势，因此可以利用concurrent.futures模块的ProcessPoolExecutor来利用多核CPU来提升执行速度。
ProcessPoolExecutor 利用multiprocessing模块提供的底层机制，在主解释器之外生成多个子解释器，然后在主解释器和子解释器之间使用socket传输参数和结果等数据，具体如下：
 把列表中或者iteration传给map，其中每一项都会传给map指定的执行函数。 用pickle对数据进行序列化，将其转成二进制。 通过本地socket，将序列化的数据从主解释器所在的进程，发送到子解释器所在的进程。 在子进程中，用pickle对二进制数据进行反序列化，将其还原成Python对象。 引入包含执行函数的Python模块。 在每个子进程中并行地对各自的输入数据进行计算。 对计算结果进行序列化操作，将其转变成字节。 将这些字节通过socket复制到主进程中。 主进程对这些字节执行反序列化操作，还原成Python对象。 最后，把每个子进程所算出的计算结果合并到一个列表中，返回给调用者。  multiprocessing开销大，因为在主进程子进程之间通信，会进行序列化和反序列化的操作。
ProcessPoolExecutor::map的实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  def map(self, fn, *iterables, **kwargs): &amp;#34;&amp;#34;&amp;#34;Returns a iterator equivalent to map(fn, iter). Args: fn: A callable that will take as many arguments as there are passed iterables.</description>
    </item>
    
    <item>
      <title>Python GIL, Multi-Threading, Multi-Processing</title>
      <link>http://localhost:1313/forgetful/2020/02/python-gil-multithread-multiprocessing/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/python-gil-multithread-multiprocessing/</guid>
      <description>References https://blog.csdn.net/AnyThingFromBigban/article/details/73611386 GIL 与 Python 线程的纠葛 GIL是什么东西？它对我们的python程序会产生什么样的影响？我们先来看一个问题。 运行下面这段python程序，CPU占用率是多少？
1 2 3 4 5 6  # 请勿在工作中模仿，危险:) def dead_loop(): while True: pass dead_loop()   答案是什么呢，占用100%CPU？那是单核！还得是没有超线程的古董CPU。 在我的双核CPU上，这个死循环只会吃掉我一个核的工作负荷，也就是只占用50%CPU。 那如何能让它在双核机器上占用100%的CPU呢？答案很容易想到，用两个线程就行了， 线程不正是并发分享CPU运算资源的吗。可惜答案虽然对了，但做起来可没那么简单。 下面的程序在主线程之外又起了一个死循环的线程。
1 2 3 4 5 6 7 8 9 10 11 12 13 14  import threading def dead_loop(): while True: pass # 新起一个死循环线程 t = threading.Thread(target=dead_loop) t.start() # 主线程也进入死循环 dead_loop() t.join()   按道理它应该能做到占用两个核的CPU资源，可是实际运行情况却是没有什么改变，还是只占了50%CPU不到。 这又是为什么呢？难道python线程不是操作系统的原生线程？ 打开system monitor一探究竟，这个占了50%的python进程确实是有两个线程在跑。 那这两个死循环的线程为何不能占满双核CPU资源呢？其实幕后的黑手就是GIL。
GIL 的迷思：痛并快乐着 GIL 的全称为 Global Interpreter Lock ，意即全局解释器锁。在 Python 语言的主流实现 CPython 中， GIL 是一个货真价实的全局线程锁，在解释器解释执行任何 Python 代码时，都需要先获得这把锁才行， 在遇到 I/O 操作时会释放这把锁。如果是纯计算的程序，没有 I/O 操作，解释器会每隔 100 次操作 就释放这把锁，让别的线程有机会执行（这个次数可以通过sys.</description>
    </item>
    
    <item>
      <title>Python Interview Questions</title>
      <link>http://localhost:1313/forgetful/2020/02/python-interview-questions/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/python-interview-questions/</guid>
      <description>Python是怎么样一种语言，它有什么特点？Golang与Python相比，有什么特点？ 如果让你用Python写一个并行执行的程序，你会怎样写？Python的多线程有什么优缺点？ 以下程序的运行结果？Why？做什么样改动可以避免类似错误？ 1 2 3 4 5 6 7 8  def f(x,l=[]): for i in range(x): l.append(i*i) print l f(2) f(3,[3,2,1]) f(3)   什么是Python装饰器？写个装饰器的例子？如果装饰器需要参数化，该如何改？ The difference between @classmethod, @staticmethod? And what is @property? 性能分析，如何证明？如何做代码分析？ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  lst = [random.random() for _ in range(10000)] def f1(lst): l1 = sorted(lst) l2 = [i for i in l1 if i&amp;lt;0.</description>
    </item>
    
    <item>
      <title>Python String Encoding</title>
      <link>http://localhost:1313/forgetful/2020/02/python-strings-encoding/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/python-strings-encoding/</guid>
      <description>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74  # coding: utf-8 # If the editor saves the source code as `utf-8` encoding, then the python interpreter # uses `coding: utf-8` to decode the source code.</description>
    </item>
    
    <item>
      <title>RAID</title>
      <link>http://localhost:1313/forgetful/2020/02/storage-raid/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/storage-raid/</guid>
      <description>RAID http://oserror.com/backend/raid-principle/ RAID的一般有如下作用  数据冗余： 指把数据的校验信息存放在冗余的磁盘中，在某些磁盘数据损坏时，能从其他未损坏的磁盘中，重新构建数据。 性能提升： 指RAID能把多块独立的磁盘组成磁盘阵列，通过把数据切成分片的方式，使得读/写数据能走多块磁盘，从而提升性能。  </description>
    </item>
    
    <item>
      <title>Raid Group and Storage Group and Storage Pool</title>
      <link>http://localhost:1313/forgetful/2020/02/storage-group-storage-pool-raid-group/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/storage-group-storage-pool-raid-group/</guid>
      <description>Raid Group 将多个硬盘组合起来的一个集合，以实现更大容量、更快读写速度、更高冗余度等目的。
位于RAID Group之上的逻辑结构称为LUN。供主机使用。
Storage Group 为了实现LUN Masking（LUN的安全屏蔽机制，即1.仅将LUN分配给特定的主机；2.阻止主机看到存储中所有的LUN），需要有一个容器来“存放”LUN与主机的关系，这个容器就是Storage Group。
先创建一个Storage Group，再连接进主机（Connect Hosts)，然后将LUN添加进这个Storage Group，主机就可以看到添加进去的LUN。
Storage Pool Pool为了实现存储虚拟化（Storage Virtualization）而诞生的。对CLARiiON来说，就是其引入的Virtual Provisioning功能。该功能可以让用户在Pool中创建Thin或者Thick LUN来分配存储资源，并且实现全自动存储分层（FAST）。
Raid Group vs Storage Pool Pool LUN并不能完全替代RAID Group LUN，如Hot Spare、Write Intent Log、Clone Private LUN、MirrorView、Clone必须要求RAID Group。
Storage Pool只是在RAID Group上做了一层抽象，底层依然是一个个的RAID Group。到Storage Pool的IO会被重定向到底层的RG，而这个重定向是通过查询抽象层所实现的表结构做到的。
单个传统RAID Group会受到16个磁盘的限制，而Storage Pool本身可含上百个的磁盘（很多个RAID Group）。
如果对性能要求严苛，并且需要物理上做到数据隔离的场合，则使用RAID Group。
https://www.dell.com/community/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E8%AE%A8%E8%AE%BA%E5%8C%BA/%E8%AF%B7%E6%95%99Raid-Group-Storage-Pool-Storage-Group%E7%AD%89%E4%B8%80%E4%BA%9B%E6%A6%82%E5%BF%B5%E7%9A%84%E5%8C%BA%E5%88%AB/td-p/6806158 </description>
    </item>
    
    <item>
      <title>Raspberrypi Aria2 Downloader</title>
      <link>http://localhost:1313/forgetful/2020/02/raspberrypi-aria2/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/raspberrypi-aria2/</guid>
      <description>UPDATE: raspberrypi is not used any more. Installation and configuration Refer to https://www.cnblogs.com/hzdx/p/raspberry_aria2.html?utm_source=itdadao&amp;utm_medium=referral NOTE I did some customization and put them in a shell. Just run it to configure aria2.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57  #!</description>
    </item>
    
    <item>
      <title>Raspberrypi Backup Files Using crontab and rsync</title>
      <link>http://localhost:1313/forgetful/2020/02/raspberrypi-crontab-rsync-back/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/raspberrypi-crontab-rsync-back/</guid>
      <description>UPDATE: raspberrypi is not used any more. Mount the disks wd_disk is usb connected to the raspberrypi.
seagate is usb connected to the router, which is exported as a NFS share.
1 2 3  pi@raspberrypi:/ $ cat /etc/fstab UUID=F474B7AA74B76DCC /mnt/wd_disk ntfs-3g rw,defaults 0 0 192.168.2.1:/mnt/Seagate_Backup_Plus_Drive /mnt/seagate nfs rw,defaults 0 0   Crontab 1 2 3 4  pi@raspberrypi:/mnt/wd_disk/backup $ sudo crontab -e # Run the command at 11:30 PM every Wendsday.</description>
    </item>
    
    <item>
      <title>Raspberrypi Syncthing</title>
      <link>http://localhost:1313/forgetful/2020/02/raspberrypi-syncthing/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/raspberrypi-syncthing/</guid>
      <description>UPDATE: raspberrypi and syncthing are not used any more.  sudo apt install syncthing sudo systemctl enable syncthing@pi sudo systemctl start sycnthing@pi  Autostart: https://docs.syncthing.net/users/autostart.html#linux NFS share is enabled on router. Then syncthing on raspberrypi could store the files from phones/PCs to the usb drive.</description>
    </item>
    
    <item>
      <title>Rate Limiting</title>
      <link>http://localhost:1313/forgetful/2020/02/rate-limiting/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/rate-limiting/</guid>
      <description>Simple Implementation  git/playground/rate-limit git/playground/rate-limiting-service  Open Source Projects There are some open source libraries which could help do the rate limiting.
Doorman https://github.com/youtube/doorman/tree/master/doc/loadtest It is used in this scenario: there is a service (RPC, REST, HTTP, .etc) having limited capacity. And you have lots of clients sending requests to the service.
It starts a doorman server. And all the clients ask for limits before sending requests to the target.</description>
    </item>
    
    <item>
      <title>Reactive Programming, Rxjava and Vert.x</title>
      <link>http://localhost:1313/forgetful/2020/02/reactive-programming-rxjava-vertx/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/reactive-programming-rxjava-vertx/</guid>
      <description>Reactive Programming https://medium.com/@mahdichtioui/reactivex-reactive-programming-principles-dbb1bafa8384 Reactive programming is programming with asynchronous data streams.
When using reactive programming, data streams are going to be the spine of your application. Events, messages, calls, and even failures are going to be conveyed by a data stream. With reactive programming, you observe these streams and react when a value is emitted.
Rxjava Reactive eXtension (http://reactivex.io , aka. RX) is an implementation of the reactive programming principles to “compose asynchronous and event-based programs by using observable sequence”.</description>
    </item>
    
    <item>
      <title>Redis</title>
      <link>http://localhost:1313/forgetful/2020/02/redis/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/redis/</guid>
      <description>Data structures  Binary-safe strings Lists Sets Sorted sets Hashes Bit arrays (bitmaps) HyperLogLogs  LRU cache Used as a caching system.
Master slave replication  async replication is used.  Persistence Several options:
 RDB: Redis Database Backup, hot snapshot that is taken periodically and is meant for point-in-time recovery. An RDB file is literally a dump of all user data stored in an internal, compressed serialization format. AOF: Append Only File, persist the dataset by taking a snapshot and appending it with changes as they arrive.</description>
    </item>
    
    <item>
      <title>Redis vs Zookeeper</title>
      <link>http://localhost:1313/forgetful/2020/02/redis-vs-zookeeper/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/redis-vs-zookeeper/</guid>
      <description>Compare Copied from stackoverflow
Redis is fast; really, really fast. It is also immediately consistent, so it&amp;rsquo;s good for fast moving data sets. The downside is that, running on one server, if it fails then you lose write access until another server takes it&amp;rsquo;s place. Replacing the server is a manual operation unless you automate it yourself. (You can still get read access to your data if you configure a slave instance).</description>
    </item>
    
    <item>
      <title>Remote Desktop to Linux</title>
      <link>http://localhost:1313/forgetful/2020/02/dev-xrdp/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/dev-xrdp/</guid>
      <description> Install xrdp. Install tigervnc-server. Use gnome session instead of classic one:  1 2 3  echo &amp;#34;gnome-session&amp;#34; &amp;gt; ~/.Xclients chmod +x ~/.Xclients sudo systemctl restart xrdp.service   Start xrdp service and make it start as system boots.  1 2  systemctl start xrdp.service systemctl enable xrdp.service   If the connection fails, it could be caused by firewall settings. Just add a new rule to firewall.  1 2  firewall-cmd --permanent --zone=public --add-port=3389/tcp firewall-cmd --reload   </description>
    </item>
    
    <item>
      <title>Set Up Shadowsocks on VPS</title>
      <link>http://localhost:1313/forgetful/2020/02/shadowsocks/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/shadowsocks/</guid>
      <description>Buy a Linux VPS service from iozoom.com The default user is root.
Install SSR https://github.com/Alvin9999/new-pac/wiki/%E8%87%AA%E5%BB%BAss%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%95%99%E7%A8%8B 1 2 3 4 5 6 7  $ wget -N --no-check-certificate https://raw.githubusercontent.com/ToyoDAdoubi/doubi/master/ssr.sh &amp;amp;&amp;amp; chmod +x ssr.sh &amp;amp;&amp;amp; bash ssr.sh $ # or $ wget --no-check-certificate https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocksR.sh $ chmod +x shadowsocksR.sh $ ./shadowsocksR.sh 2&amp;gt;&amp;amp;1 | tee shadowsocksR.log   Modify SSR 1  $ bash ssr.sh   [Do NOT use this, python shadowsocks not maintained, too old] Bring shadowsocks online 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  # Install $ apt-get install python-pip $ pip install shadowsocks # Do some configuration and start service $ ssserver -h $ vim /etc/shadowsocks.</description>
    </item>
    
    <item>
      <title>Sharding Pinterest</title>
      <link>http://localhost:1313/forgetful/2020/02/sharding-pinterest/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/sharding-pinterest/</guid>
      <description>Note of https://medium.com/@Pinterest_Engineering/sharding-pinterest-how-we-scaled-our-mysql-fleet-3f341e96ca6f Feature Required  Create Pins. Like other Pins. Follow other Pinners. View a home feed of all the Pinners he/she follows. Support asking for N number of Pins in a board in a deterministic order (such as reverse creation time or user specified ordering).  Scale Required 50M Pins have been saved by Pinners onto 1B boards.
Design Goals  Stable and high available. Low latency. Eventually consistency.</description>
    </item>
    
    <item>
      <title>Sharding Pinterest: How We Scaled Our MySQL Fleet</title>
      <link>http://localhost:1313/forgetful/2020/02/design-sharding-pinterest/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/design-sharding-pinterest/</guid>
      <description>https://medium.com/@Pinterest_Engineering/sharding-pinterest-how-we-scaled-our-mysql-fleet-3f341e96ca6f TODO</description>
    </item>
    
    <item>
      <title>Some Auto Setup by Devstack</title>
      <link>http://localhost:1313/forgetful/2020/02/openstack-devstack-auto-create/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/openstack-devstack-auto-create/</guid>
      <description>Neutron network part 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54  openstack --os-cloud devstack-admin --os-region RegionOne subnet pool create \  shared-default-subnetpool-v4 --default-prefix-length 26 \  --pool-prefix 10.</description>
    </item>
    
    <item>
      <title>Some Design Thoughts</title>
      <link>http://localhost:1313/forgetful/2020/02/design-thoughts/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/design-thoughts/</guid>
      <description>SQL or NoSQL SQL Tables vs NoSQL Documents SQL databases store data records in tables.
NoSQL databases store JSON-like field-value pair documents. Similar documents can be stored in a collection, which is analogous to an SQL table.
SQL Schema vs NoSQL Schemaless SQL Normalization vs NoSQL Denormalization SQL databases use foreign key to relate one table to another, which minimizes data redundancy.
We can use normalization in NoSQL, but this is not always practical.</description>
    </item>
    
    <item>
      <title>Spearman&#39;s Rank Correlation</title>
      <link>http://localhost:1313/forgetful/2020/02/statistics-spearmans-rank-order-correlation/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/statistics-spearmans-rank-order-correlation/</guid>
      <description>https://statistics.laerd.com/statistical-guides/spearmans-rank-order-correlation-statistical-guide-2.php  Definition The Spearman correlation coefficient, rs, can take values from +1 to -1. A rs of +1 indicates a perfect association of ranks, a rs of zero indicates no association between ranks and a rs of -1 indicates a perfect negative association of ranks. The closer rs is to zero, the weaker the association between the ranks.</description>
    </item>
    
    <item>
      <title>System Design</title>
      <link>http://localhost:1313/forgetful/2020/02/system-design/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/system-design/</guid>
      <description>No design is correct or wrong. There are just good designs and bad designs which heavily depend on the use case.
Hence, it is extremely important to clarify the requirements for the problem asked.
Basic Terminologies Replication Frequently copying the data across multiple machines.
Consistency The data is same across the cluster with multiple machines. You can read or write to/from any node and get the same data.
Eventual Consistency All machines will have the same data eventually.</description>
    </item>
    
    <item>
      <title>System Design Facebook Interview</title>
      <link>http://localhost:1313/forgetful/2020/02/design-facebook-interview/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/design-facebook-interview/</guid>
      <description>Guideline  Should not assume anything. Pin down the requirements, they&amp;rsquo;re looking for me to drive conversation. It should be my design, not half mine and half the interviewers. Try to cover both breadth and depth. Need to talk about both high level concepts and details associated.  What is Facebook looking for  They want to understand how I reason through a problem that I&amp;rsquo;ve not necessarily encountered before. They are looking to get signal on both my techincal and communication skills.</description>
    </item>
    
    <item>
      <title>System Design Topics</title>
      <link>http://localhost:1313/forgetful/2020/02/design-topics/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/design-topics/</guid>
      <description>Some high-level trade-offs  Performance vs scalability Latency vs throughput Availability vs consistency  Keep in mind that everything is a trade-off.
Performance vs scalability A service is scalable if it results in increased performance in a manner proportional to resources added. Generally, increasing performance means serving more units of work, but it can also be to handle larger units of work, such as when datasets grow.
Many algorithms that perform reasonably well under low load and small datasets can explode in cost if either requests rates increase, the dataset grows or the number of nodes in the distributed system increases.</description>
    </item>
    
    <item>
      <title>The Rust Programming Language</title>
      <link>http://localhost:1313/forgetful/2020/02/the-rust-programming-language/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/the-rust-programming-language/</guid>
      <description>Chap 3 Common Programming Concept 3.1 Variables and Mutability By default variables are immutable.
Immutable variables vs. Constants: Constants are valid for the entire time a program runs, within the scope they were declared in, making them a useful choice for values in your application domain that multiple parts of the program might need to know about.
Shadowing: We can shadow a variable by using the same variable’s name and repeating the use of the let keyword.</description>
    </item>
    
    <item>
      <title>Tooz</title>
      <link>http://localhost:1313/forgetful/2020/02/tooz/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/tooz/</guid>
      <description>Document page: https://docs.openstack.org/tooz/latest/ Tooz is a library to manage the distributed primitives like group membership protocol, lock service and leader election. It provides a common coordination API for different drivers, like zookeeper, redis, memcached, etcd, .etc.
Drivers Document page: https://docs.openstack.org/tooz/latest/user/drivers.html Zookeeper The zookeeper is the reference implementation and provides the most solid features as it&amp;rsquo;s possible to build a cluster of zookeeper servers that is resilient towards network partitions for example.</description>
    </item>
    
    <item>
      <title>Ubuntu Cloud Image Installation</title>
      <link>http://localhost:1313/forgetful/2020/02/ubuntu-clould-image/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/ubuntu-clould-image/</guid>
      <description>Download https://cloud-images.ubuntu.com/ Upload image to Glance Boot an instance from image NOTE: don&amp;rsquo;t forget to use the ssh key.
Initial configuration SSH to the instance 1  $ ssh -i ~/.ssh/id_rsa.kolla-ansible.liangr ubuntu@172.30.1.18   Create the user stack 1 2 3  $ sudo useradd -m -s /bin/bash -G sudo stack $ sudo passwd stack   Modify /etc/ssh/sshd_config to allow login via password Entry name is PasswordAuthentication.</description>
    </item>
    
    <item>
      <title>Ubuntu Remove the Shortcuts of Ctrl&#43;Alt&#43;Left and Ctrl&#43;Alt&#43;Right</title>
      <link>http://localhost:1313/forgetful/2020/02/remove-shortcuts-ctrl-alt-left-right/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/remove-shortcuts-ctrl-alt-left-right/</guid>
      <description>In Ubuntu, PyCharm conflicts with the gnome desktop shortcuts, which makes key-bindings Ctrl+Alt+Left and Ctrl+Alt+Right not work.
Use below commands to remove the shortcuts from gnome desktop.
1 2 3 4 5 6 7 8  # List the current shortcuts. liangr@ubuntu-dev:~$ gsettings list-recursively org.gnome.desktop.wm.keybindings | grep Left org.gnome.desktop.wm.keybindings move-to-monitor-left [&amp;#39;&amp;lt;Super&amp;gt;&amp;lt;Shift&amp;gt;Left&amp;#39;] org.gnome.desktop.wm.keybindings move-to-workspace-left [&amp;#39;&amp;lt;Control&amp;gt;&amp;lt;Shift&amp;gt;&amp;lt;Alt&amp;gt;Left&amp;#39;] org.gnome.desktop.wm.keybindings switch-to-workspace-left [&amp;#39;&amp;lt;Control&amp;gt;&amp;lt;Alt&amp;gt;Left&amp;#39;] liangr@ubuntu-dev:~$ gsettings set org.gnome.desktop.wm.keybindings switch-to-workspace-left &amp;#34;[]&amp;#34; liangr@ubuntu-dev:~$ gsettings set org.gnome.desktop.wm.keybindings switch-to-workspace-right &amp;#34;[]&amp;#34;   </description>
    </item>
    
    <item>
      <title>Unix Domain Socket</title>
      <link>http://localhost:1313/forgetful/2020/02/uds/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/uds/</guid>
      <description>When it comes to inter-process communication (IPC) between processes on the same Linux host, there are multiple options:
 FIFOs pipes shared memory sockets and so on  Unix Domain Sockets combine the convinient API of sockets with higher performance of other single-host methods.
UDS supports streams (TCP equivalent) and datagrams (UDP equivalent).
IPC with UDS looks very similar to IPC with regular TCP sockets using the loop-back interface (localhost or 127.</description>
    </item>
    
    <item>
      <title>V2Ray and Related</title>
      <link>http://localhost:1313/forgetful/2020/02/v2ray/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/v2ray/</guid>
      <description>V2Ray and Others  FQ V2Ray 加速 BBR 伪装  免费域名 websocket+tls+web CDN    FQ V2Ray 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  # # Run as root on VPS # bash &amp;lt;(curl -L https://raw.githubusercontent.com/v2fly/fhs-install-v2ray/master/install-release.sh) # # Below go.sh is deprecated. Refer  # # https://github.</description>
    </item>
    
    <item>
      <title>VAST</title>
      <link>http://localhost:1313/forgetful/2020/02/vast/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/vast/</guid>
      <description>VAST Data has found a way to collapse all storage tiers onto one with decoupled compute nodes using NVMeoF to access 2U databoxes filled with QLC flash data drives and Optane XPoint metadata and write-staging drives, with up to 2PB or more of capacity after data reduction.
3D XPoint memory is non-volatile, faster than NAND but slower than DRAM.
There is a shared-everything architecture embodied in separate and independently scalable compute nodes in a loosely-coupled cluster running Universal Filesystem logic.</description>
    </item>
    
    <item>
      <title>What Even Is A Container And A Kubernetes Pod</title>
      <link>http://localhost:1313/forgetful/2020/02/kubernetes-container-pod-what-it-is/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/kubernetes-container-pod-what-it-is/</guid>
      <description>Container The word container doesn&amp;rsquo;t mean anything super precise. Basically there are a few new Linux kernel features (namespaces and cgroups) that let you isolate processes from each other. When you use those features, you call it containers.
Basically these features let you pretend you have something like a virtual machine, except it&amp;rsquo;s not a virtual machine at all, it&amp;rsquo;s just processes running in the same Linux kernel.
Namespaces namespaces is a Linux feature which could separate your processes from the other processes on the same computer.</description>
    </item>
    
    <item>
      <title>Yum Localinstall</title>
      <link>http://localhost:1313/forgetful/2020/02/yum-localinstall/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/yum-localinstall/</guid>
      <description>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82  [root@4994829af1cb osp-deploy]# yum localinstall .</description>
    </item>
    
    <item>
      <title>Zookeeper</title>
      <link>http://localhost:1313/forgetful/2020/02/zookeeper/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/zookeeper/</guid>
      <description>https://stackoverflow.com/questions/37293928/zookeeper-vs-in-memory-data-grid-vs-redis By default, Zookeeper replicates all your data to every node and lets clients watch the data for changes. Changes are sent very quickly (within a bounded amount of time) to clients. You can also create &amp;ldquo;ephemeral nodes&amp;rdquo;, which are deleted within a specified time if a client disconnects. ZooKeeper is highly optimized for reads, while writes are very slow (since they generally are sent to every client as soon as the write takes place).</description>
    </item>
    
    <item>
      <title>大话存储</title>
      <link>http://localhost:1313/forgetful/2020/02/%E5%A4%A7%E8%AF%9D%E5%AD%98%E5%82%A8/</link>
      <pubDate>Mon, 10 Feb 2020 16:37:31 +0800</pubDate>
      
      <guid>http://localhost:1313/forgetful/2020/02/%E5%A4%A7%E8%AF%9D%E5%AD%98%E5%82%A8/</guid>
      <description>2. IO大法 主板上每个部件都是通过总线连接起来。
网络通信：
 连-物理层 找-数据链路和网络层。设备区分，编址如IP 发-上三层。发什么数据，数据格式是什么  CPU向磁盘要数据：
  连：系统总线，北桥，南桥（IDE控制器，USB控制器），PCI总线
  找：每个IO设备在启动时都要向内存中映射一个或多个地址。针对这个地址的数据，统统被北桥芯片重定向到总线上实际的设备上。例如IDE磁盘控制器地址被映射到地址0xA0。
  发：CPU把这个IO地址放到系统总线上，北桥接收到之后，会等待CPU发送第一个针对这个外设的指令。CPU发送如下3条指令：
 当前操作是读还是写，其他选项 指明应该读取的硬盘逻辑块号（LBA） 给出了读取出来的内容应该存放到内存中的哪个地址中。有了这个地址，数据读出后直接通过DMA技术，磁盘控制器可以直接对内存寻址并执行写操作，而不必先转到CPU，然后再从CPU存到内存中。  这三条指令被北桥依次发送给IO总线上的磁盘控制器来执行。这些控制器一般是集成在南桥上或者通过PCI接入IO总线，比如ATA控制器，SCSI控制器。CPU只需要通过运行控制器驱动程序，拿到逻辑块地址，并将读或写信号发给磁盘控制器，磁盘控制器再向磁盘发出一系列指令，让磁盘进行某个磁道或者扇区的读写。
磁盘控制器指令主要有ATA指令集和SCSI指令集。指令集也称为协议，这样通信双方知道对方传过来的比特流里面到底包含了什么。
  3. 磁盘大挪移  用于ATA指令系统的IDE接口 用于ATA指令系统的SATA接口 用于SCSI指令系统的并行SCSI接口 用于SCSI指令系统的串行SCSI(SAS)接口 用于SCSI指令系统的IBM专用串行SCSI接口（SSA） 用于SCSI指令系统的并且承载于FabreChannel协议的串行FC接口（FCP）  SCSI硬盘接口 必须有专门的SCSI控制器，也就是一块SCSI控制卡，控制器上有一个相当于CPU的芯片，它对SCSI设备进行控制，能处理大部分的工作，减少CPU的负担。
SCSI协议的链路层 保证数据帧成功地传送到这条线路的对端。SCSI协议利用CRC校验码来校验每个指令或者数据的帧，对方发来的校验码与本地计算的不同，则丢弃。发送方便会重传这个帧。
SCSI协议的网络层 SCSI总线的寻址方式：控制器-通道-SCSI ID-LUN ID。
一个控制器可以控制多个通道，每个通道上可以连接多个SCSI设备，每个SCSI设备在逻辑上可以划分出若干个LUN。
SCSI协议的传输层 保障此端的数据成功地传送到彼端。与链路层不同的是，链路层只是保障线路两端数据的传送，而且一旦某个帧出错，链路层程序本身不会重新传送这个帧。
磁盘控制器驱动程序 磁盘控制器将底层机制隐藏，向驱动程序提供一种简洁的接口。驱动程序只要将要读写的设备号，起始地址等信息，也就是指令描述块（CDB）传递给控制器即可。
IOPS 磁盘的IOPS，每秒能进行多少次IO，每次IO根据写入数据的大小，这个值也不是固定的。
 如果在不频繁换道的情况下，每次IO都写入很大的一块连续数据，则此时每秒所做的IO次数是比较低的 如果磁头频繁换道，每次写入的数据还比较大的话，此时的IOPS应该是这块磁盘的最低数值 如果在不频繁换道的情况下，每次写入最小的数据块，则此时IOPS将是最高值。如果IO的payload长度为0,不包含开销，此时的IOPS则为理论最大极限值  例如，写入10000个大小为1KB的文件到硬盘上，耗费的时间比写入一个10MB大小的文件多得多，虽然数据总量都是10MB。因为写入10000个文件时，根据文件分布情况和大小情况，可能需要做好几万甚至十几万次IO才能完成。而写入一个10MB的大文件，如果这个文件在磁盘上是连续存放的，那么只需要几十个IO就可以完成。
对于写入10000个小文件的情况，因为每秒所需的IO非常高，所以如果用具有较高IOPS的磁盘，会提速不少。
然而写入一个10MB的文件，就算使用了较高IOPS的磁盘来做，也不会有提升，因为只需很少的IO就完成了，只有换用较大传输带宽（吞吐量）的硬盘，才能体现出优势。
传输带宽/吞吐量 如果写入10000个1KB的文件花了10s，那么此时的传输带宽只能达到1MB/s，而写入一个10MB的文件只花了0.1s，那么此时的传输带宽就是100MB/s。所以，即使同一块硬盘在写入不同大小的数据时，表现出来的带宽也是不同的。
具有高带宽的硬盘在传输大块连续数据时具有优势，而具有高IOPS的硬盘在传输小块不连续的数据时具有优势。
Flash芯片的通病   Erase Before Overwrite</description>
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/forgetful/1/01/openstack-devstack-setup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/forgetful/1/01/openstack-devstack-setup/</guid>
      <description>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361  $ sudo useradd -m -s /bin/bash -G sudo stack $ sudo passwd stack $ # use welcome as the password.</description>
    </item>
    
  </channel>
</rss>
